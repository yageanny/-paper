{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #基础包\n",
    "import numpy as np #基础包\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdate \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns \n",
    "from sklearn.datasets import fetch_olivetti_faces #人脸数据库\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "import re\n",
    "import jieba\n",
    "import pandas as pd #基础包\n",
    "import numpy as np #基础包\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdate \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns \n",
    "from sklearn.datasets import fetch_olivetti_faces #人脸数据库\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "#from transformers.convert_graph_to_onnx import *\n",
    "douyin = pd.read_csv(\"douyin.csv\",encoding=\"utf-8\",index_col = 0)\n",
    "stopwords = [line.strip() for line in open('stopwords.txt',encoding='UTF-8').readlines()]\n",
    "#stopwords = [line.strip() for line in open('F:\\\\2023暑期班材料\\\\stopwords0.txt',encoding='UTF-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              # 为中国点赞👍，为我们医务工作者点赞👍，#抗疫#抖音小助手 #穿越时空只想见你\n",
       "1       #2020 盘点2020那些事儿，“20个数字穿越2020” #抗疫  #火神山  #嫦娥五号\n",
       "2                                #2020上海宝马工程机械展 上海加油#抗疫\n",
       "3            #4月4日全国哀悼 柘木所全体民辅警哀悼#抗疫 烈士和逝世同胞@平安湖北 @荆州公安\n",
       "4              #NZ#lockdown#stayhome 老外写中文 #抗疫#快乐宅家同心抗疫\n",
       "                             ...                       \n",
       "4021    🎉🎉恭喜#竹溪 县#妇幼保健院 #抗疫 #天使 👼🏻黄霞，#荣获 #十堰市 五一劳动奖章 🎉🎉\n",
       "4022                          🎨《这是一个画画的文案》#抗疫应援 #抗疫 #手绘\n",
       "4023                      🐴住！冬季抗疫宝典👍🏻#抗疫 #防护措施 @DOU+小助手\n",
       "4024         👏会跳舞的抗疫医护宝宝来报道啦，😘大家想看看跳舞的样子吗？有请天使宝宝出场❤️#抗疫\n",
       "4025              😈三条尾巴的老鼠见过嘛🐭#本命 #新冠病毒 #抗疫 #trantision\n",
       "Name: script, Length: 4026, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "douyin.head()\n",
    "douyin['script'].shape\n",
    "douyin['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中国', '点赞', '医务', '工作者', '点赞', '抗疫', '穿越时空', '想见']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#一个清理文本的函数\n",
    "def clean_text(text):\n",
    "    text  = text.replace('\\n',\" \")\n",
    "    text = re.sub('-',\" \",text)\n",
    "    text = re.sub(' ',\"\",text)\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) #日期，对主体模型没什么意义\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) #时间，没意义\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) #邮件地址，没意义\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text) #网址，没意义\n",
    "    pure_text = ''\n",
    "    for letter in text:\n",
    "        if letter.isalpha() or letter ==' ':\n",
    "            pure_text += letter\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n",
    "    return text#可以自己处理内容\n",
    "\n",
    "#docs是清理之后的一句一句话\n",
    "docs = douyin[\"script\"]\n",
    "docs = docs.apply(lambda x :clean_text(x))#对docs的每个元素执行clean_text函数\n",
    "docs[0]\n",
    "#将一句一句话分割成词\n",
    "texts = []\n",
    "for doc in docs: \n",
    "    doc = str(doc)\n",
    "    #text = jieba.lcut(doc)\n",
    "    text = jieba.lcut(doc)\n",
    "    text = [w for w in text if w not in stopwords]\n",
    "    texts.append(list(text))\n",
    "texts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =corpora.Dictionary(texts) #将text编成字典的形式储存\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(\"字典的第一个词dictionary[0]：\",dictionary[0])\n",
    "print(\"文本数据中的第一句话docs[0]：\",docs[0])\n",
    "print(\"第一句话分词后的结果texts[0]：\",texts[0])\n",
    "\n",
    "#左边的数字代表词典中的某个词（比如0代表abandon，999代表zoo），\n",
    "#右边的数字代表这个词在文档中出现的频数，\n",
    "#（0，0）代表abandon这个词在文档中出现了0次，\n",
    "#（999, 2）代表zoo这个词在文档中出现了2次\n",
    "print(\"第一句话语料化后的结果corpus[0]：\",corpus[0])\n",
    "#print(\"第二句话语料化后的结果corpus[1]：\",corpus[1])\n",
    "\n",
    "#注意，texts和corpus的数目是不对应的，因为texts包括重复词，而corpus不包括\n",
    "print(len(texts[0]),len(corpus[0]))\n",
    "#tf_idf作为一种文本的加权手段，突出词汇的重要性\n",
    "#tf_idf = word_freq * log（全部文本数 / 包含该词的文本数）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = TfidfModel(corpus, normalize=False)\n",
    "\n",
    "corpus_tfidf = models.TfidfModel(corpus)#计算结果\n",
    "\n",
    "word_tf_tdf = list(tf_idf_model[corpus])#将结果存成列表\n",
    "\n",
    "df_tf_idf = DataFrame(word_tf_tdf)#将列表存成数据框\n",
    "\n",
    "#print('词典:', dictionary.token2id)\n",
    "#print('词频:', corpus)\n",
    "#print('词的tf-idf值:', word_tf_tdf)\n",
    "print(\"第一句话分词结果texts\"+ str(texts[0])+\"\\n\" +\"第一句话语料化后的结果corpus\"+str(corpus[0])+\"\\n\" +\"第一句话tf-idf后的结果word_tf_tdf\"+str(word_tf_tdf[0]))\n",
    "#print(corpus_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算积极词汇的平均向量\n",
    "pos = [\"点赞\",\"加油\",\"开心\",\"快乐\",\"不败\",\"不朽\",\"渡过难关\",\"致敬\"]\n",
    "neg = [\"哀悼\",\"不幸\",\"放弃\",\"惨烈\",\"恶意中伤\",\"抢救无效\",\"去世\",\"天堂\",\"病痛\",\"默哀\",\"确诊\",\"伤心\"]\n",
    "k1 = model.wv[pos[0]]\n",
    "for i in range(1,len(pos)):\n",
    "    k1 = k1 + model.wv[pos[i]]\n",
    "num_pos = len(pos)\n",
    "k_pos = k1/num_pos #计算所有积极词汇向量的均值\n",
    "\n",
    "#计算消极词汇的平均向量\n",
    "k2 = model.wv[neg[0]]\n",
    "for i in range(1,len(neg)):\n",
    "    k2 = k2 + model.wv[neg[i]]\n",
    "num_neg = len(neg)\n",
    "k_neg = k2/num_neg #计算所有消极词汇向量的均值\n",
    "import numpy as np\n",
    "\n",
    "# 初始化一个空列表来存储每个词语的情感得分\n",
    "emotion_scores = []\n",
    "\n",
    "# 遍历所有词语，计算情感得分并添加到列表中\n",
    "for word in words:\n",
    "    score = emo(word)[1]  # 假设emo函数返回的是一个情感得分\n",
    "    emotion_scores.append(score)\n",
    "\n",
    "# 将列表转换为NumPy数组，以便于计算平均值\n",
    "emotion_scores_array = np.array(emotion_scores)\n",
    "\n",
    "# 计算情感得分的平均值\n",
    "average_emotion_score = np.mean(emotion_scores_array)\n",
    "\n",
    "# 打印平均情感得分\n",
    "print(\"Average Emotion Score:\", average_emotion_score)\n",
    "#一个用于计算词语情感得分的函数\n",
    "def emo(w):\n",
    "    set_seed = 888\n",
    "    word = model.wv[w]\n",
    "    direction1 = k_pos \n",
    "    direction2 = -k_neg \n",
    "    dian1 = np.dot(word,direction1) #词语在积极\n",
    "    dian2 = np.dot(word,direction2) #消极这个维度上的投影\n",
    "    sim1_result = dian1/(np.linalg.norm(word)*np.linalg.norm(direction1))\n",
    "    sim2_result = dian2/(np.linalg.norm(word)*np.linalg.norm(direction2))\n",
    "    result1 = \"【\" + str(w) + \"】\" + \"积极情感得分为：\" + str(sim1_result) \n",
    "    result2 = \"【\" + str(w) + \"】\" + \"消极情感得分为：\" + str(sim2_result) \n",
    "    return result1,result2,sim1_result,sim2_result\n",
    "print(str(emo(\"胜利\")[0])+ \"\\n\" + str(emo(\"胜利\")[1]))\n",
    "print(str(emo(\"疫情\")[0])+ \"\\n\" + str(emo(\"疫情\")[1]))\n",
    "print(str(emo(\"美国\")[0])+ \"\\n\" + str(emo(\"美国\")[1]))\n",
    "print(str(emo(\"美国佬\")[0])+ \"\\n\" + str(emo(\"美国佬\")[1]))\n",
    "print(str(emo(\"医生\")[0])+ \"\\n\" + str(emo(\"医生\")[1]))\n",
    "print(str(emo(\"死者\")[0])+ \"\\n\" + str(emo(\"死者\")[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

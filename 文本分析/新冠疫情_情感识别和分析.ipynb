{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #åŸºç¡€åŒ…\n",
    "import numpy as np #åŸºç¡€åŒ…\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdate \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns \n",
    "from sklearn.datasets import fetch_olivetti_faces #äººè„¸æ•°æ®åº“\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "import re\n",
    "import jieba\n",
    "import pandas as pd #åŸºç¡€åŒ…\n",
    "import numpy as np #åŸºç¡€åŒ…\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdate \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns \n",
    "from sklearn.datasets import fetch_olivetti_faces #äººè„¸æ•°æ®åº“\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "#from transformers.convert_graph_to_onnx import *\n",
    "douyin = pd.read_csv(\"douyin.csv\",encoding=\"utf-8\",index_col = 0)\n",
    "stopwords = [line.strip() for line in open('stopwords.txt',encoding='UTF-8').readlines()]\n",
    "#stopwords = [line.strip() for line in open('F:\\\\2023æš‘æœŸç­ææ–™\\\\stopwords0.txt',encoding='UTF-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              # ä¸ºä¸­å›½ç‚¹èµğŸ‘ï¼Œä¸ºæˆ‘ä»¬åŒ»åŠ¡å·¥ä½œè€…ç‚¹èµğŸ‘ï¼Œ#æŠ—ç–«#æŠ–éŸ³å°åŠ©æ‰‹ #ç©¿è¶Šæ—¶ç©ºåªæƒ³è§ä½ \n",
       "1       #2020 ç›˜ç‚¹2020é‚£äº›äº‹å„¿ï¼Œâ€œ20ä¸ªæ•°å­—ç©¿è¶Š2020â€ #æŠ—ç–«  #ç«ç¥å±±  #å«¦å¨¥äº”å·\n",
       "2                                #2020ä¸Šæµ·å®é©¬å·¥ç¨‹æœºæ¢°å±• ä¸Šæµ·åŠ æ²¹#æŠ—ç–«\n",
       "3            #4æœˆ4æ—¥å…¨å›½å“€æ‚¼ æŸ˜æœ¨æ‰€å…¨ä½“æ°‘è¾…è­¦å“€æ‚¼#æŠ—ç–« çƒˆå£«å’Œé€ä¸–åŒèƒ@å¹³å®‰æ¹–åŒ— @è†å·å…¬å®‰\n",
       "4              #NZ#lockdown#stayhome è€å¤–å†™ä¸­æ–‡ #æŠ—ç–«#å¿«ä¹å®…å®¶åŒå¿ƒæŠ—ç–«\n",
       "                             ...                       \n",
       "4021    ğŸ‰ğŸ‰æ­å–œ#ç«¹æºª å¿#å¦‡å¹¼ä¿å¥é™¢ #æŠ—ç–« #å¤©ä½¿ ğŸ‘¼ğŸ»é»„éœï¼Œ#è£è· #åå °å¸‚ äº”ä¸€åŠ³åŠ¨å¥–ç«  ğŸ‰ğŸ‰\n",
       "4022                          ğŸ¨ã€Šè¿™æ˜¯ä¸€ä¸ªç”»ç”»çš„æ–‡æ¡ˆã€‹#æŠ—ç–«åº”æ´ #æŠ—ç–« #æ‰‹ç»˜\n",
       "4023                      ğŸ´ä½ï¼å†¬å­£æŠ—ç–«å®å…¸ğŸ‘ğŸ»#æŠ—ç–« #é˜²æŠ¤æªæ–½ @DOU+å°åŠ©æ‰‹\n",
       "4024         ğŸ‘ä¼šè·³èˆçš„æŠ—ç–«åŒ»æŠ¤å®å®æ¥æŠ¥é“å•¦ï¼ŒğŸ˜˜å¤§å®¶æƒ³çœ‹çœ‹è·³èˆçš„æ ·å­å—ï¼Ÿæœ‰è¯·å¤©ä½¿å®å®å‡ºåœºâ¤ï¸#æŠ—ç–«\n",
       "4025              ğŸ˜ˆä¸‰æ¡å°¾å·´çš„è€é¼ è§è¿‡å˜›ğŸ­#æœ¬å‘½ #æ–°å† ç—…æ¯’ #æŠ—ç–« #trantision\n",
       "Name: script, Length: 4026, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "douyin.head()\n",
    "douyin['script'].shape\n",
    "douyin['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ä¸­å›½', 'ç‚¹èµ', 'åŒ»åŠ¡', 'å·¥ä½œè€…', 'ç‚¹èµ', 'æŠ—ç–«', 'ç©¿è¶Šæ—¶ç©º', 'æƒ³è§']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ä¸€ä¸ªæ¸…ç†æ–‡æœ¬çš„å‡½æ•°\n",
    "def clean_text(text):\n",
    "    text  = text.replace('\\n',\" \")\n",
    "    text = re.sub('-',\" \",text)\n",
    "    text = re.sub(' ',\"\",text)\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) #æ—¥æœŸï¼Œå¯¹ä¸»ä½“æ¨¡å‹æ²¡ä»€ä¹ˆæ„ä¹‰\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) #æ—¶é—´ï¼Œæ²¡æ„ä¹‰\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) #é‚®ä»¶åœ°å€ï¼Œæ²¡æ„ä¹‰\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text) #ç½‘å€ï¼Œæ²¡æ„ä¹‰\n",
    "    pure_text = ''\n",
    "    for letter in text:\n",
    "        if letter.isalpha() or letter ==' ':\n",
    "            pure_text += letter\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n",
    "    return text#å¯ä»¥è‡ªå·±å¤„ç†å†…å®¹\n",
    "\n",
    "#docsæ˜¯æ¸…ç†ä¹‹åçš„ä¸€å¥ä¸€å¥è¯\n",
    "docs = douyin[\"script\"]\n",
    "docs = docs.apply(lambda x :clean_text(x))#å¯¹docsçš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œclean_textå‡½æ•°\n",
    "docs[0]\n",
    "#å°†ä¸€å¥ä¸€å¥è¯åˆ†å‰²æˆè¯\n",
    "texts = []\n",
    "for doc in docs: \n",
    "    doc = str(doc)\n",
    "    #text = jieba.lcut(doc)\n",
    "    text = jieba.lcut(doc)\n",
    "    text = [w for w in text if w not in stopwords]\n",
    "    texts.append(list(text))\n",
    "texts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =corpora.Dictionary(texts) #å°†textç¼–æˆå­—å…¸çš„å½¢å¼å‚¨å­˜\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(\"å­—å…¸çš„ç¬¬ä¸€ä¸ªè¯dictionary[0]ï¼š\",dictionary[0])\n",
    "print(\"æ–‡æœ¬æ•°æ®ä¸­çš„ç¬¬ä¸€å¥è¯docs[0]ï¼š\",docs[0])\n",
    "print(\"ç¬¬ä¸€å¥è¯åˆ†è¯åçš„ç»“æœtexts[0]ï¼š\",texts[0])\n",
    "\n",
    "#å·¦è¾¹çš„æ•°å­—ä»£è¡¨è¯å…¸ä¸­çš„æŸä¸ªè¯ï¼ˆæ¯”å¦‚0ä»£è¡¨abandonï¼Œ999ä»£è¡¨zooï¼‰ï¼Œ\n",
    "#å³è¾¹çš„æ•°å­—ä»£è¡¨è¿™ä¸ªè¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘æ•°ï¼Œ\n",
    "#ï¼ˆ0ï¼Œ0ï¼‰ä»£è¡¨abandonè¿™ä¸ªè¯åœ¨æ–‡æ¡£ä¸­å‡ºç°äº†0æ¬¡ï¼Œ\n",
    "#ï¼ˆ999, 2ï¼‰ä»£è¡¨zooè¿™ä¸ªè¯åœ¨æ–‡æ¡£ä¸­å‡ºç°äº†2æ¬¡\n",
    "print(\"ç¬¬ä¸€å¥è¯è¯­æ–™åŒ–åçš„ç»“æœcorpus[0]ï¼š\",corpus[0])\n",
    "#print(\"ç¬¬äºŒå¥è¯è¯­æ–™åŒ–åçš„ç»“æœcorpus[1]ï¼š\",corpus[1])\n",
    "\n",
    "#æ³¨æ„ï¼Œtextså’Œcorpusçš„æ•°ç›®æ˜¯ä¸å¯¹åº”çš„ï¼Œå› ä¸ºtextsåŒ…æ‹¬é‡å¤è¯ï¼Œè€Œcorpusä¸åŒ…æ‹¬\n",
    "print(len(texts[0]),len(corpus[0]))\n",
    "#tf_idfä½œä¸ºä¸€ç§æ–‡æœ¬çš„åŠ æƒæ‰‹æ®µï¼Œçªå‡ºè¯æ±‡çš„é‡è¦æ€§\n",
    "#tf_idf = word_freq * logï¼ˆå…¨éƒ¨æ–‡æœ¬æ•° / åŒ…å«è¯¥è¯çš„æ–‡æœ¬æ•°ï¼‰\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = TfidfModel(corpus, normalize=False)\n",
    "\n",
    "corpus_tfidf = models.TfidfModel(corpus)#è®¡ç®—ç»“æœ\n",
    "\n",
    "word_tf_tdf = list(tf_idf_model[corpus])#å°†ç»“æœå­˜æˆåˆ—è¡¨\n",
    "\n",
    "df_tf_idf = DataFrame(word_tf_tdf)#å°†åˆ—è¡¨å­˜æˆæ•°æ®æ¡†\n",
    "\n",
    "#print('è¯å…¸:', dictionary.token2id)\n",
    "#print('è¯é¢‘:', corpus)\n",
    "#print('è¯çš„tf-idfå€¼:', word_tf_tdf)\n",
    "print(\"ç¬¬ä¸€å¥è¯åˆ†è¯ç»“æœtexts\"+ str(texts[0])+\"\\n\" +\"ç¬¬ä¸€å¥è¯è¯­æ–™åŒ–åçš„ç»“æœcorpus\"+str(corpus[0])+\"\\n\" +\"ç¬¬ä¸€å¥è¯tf-idfåçš„ç»“æœword_tf_tdf\"+str(word_tf_tdf[0]))\n",
    "#print(corpus_tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æƒ…æ„Ÿåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è®¡ç®—ç§¯æè¯æ±‡çš„å¹³å‡å‘é‡\n",
    "pos = [\"ç‚¹èµ\",\"åŠ æ²¹\",\"å¼€å¿ƒ\",\"å¿«ä¹\",\"ä¸è´¥\",\"ä¸æœ½\",\"æ¸¡è¿‡éš¾å…³\",\"è‡´æ•¬\"]\n",
    "neg = [\"å“€æ‚¼\",\"ä¸å¹¸\",\"æ”¾å¼ƒ\",\"æƒ¨çƒˆ\",\"æ¶æ„ä¸­ä¼¤\",\"æŠ¢æ•‘æ— æ•ˆ\",\"å»ä¸–\",\"å¤©å ‚\",\"ç—…ç—›\",\"é»˜å“€\",\"ç¡®è¯Š\",\"ä¼¤å¿ƒ\"]\n",
    "k1 = model.wv[pos[0]]\n",
    "for i in range(1,len(pos)):\n",
    "    k1 = k1 + model.wv[pos[i]]\n",
    "num_pos = len(pos)\n",
    "k_pos = k1/num_pos #è®¡ç®—æ‰€æœ‰ç§¯æè¯æ±‡å‘é‡çš„å‡å€¼\n",
    "\n",
    "#è®¡ç®—æ¶ˆæè¯æ±‡çš„å¹³å‡å‘é‡\n",
    "k2 = model.wv[neg[0]]\n",
    "for i in range(1,len(neg)):\n",
    "    k2 = k2 + model.wv[neg[i]]\n",
    "num_neg = len(neg)\n",
    "k_neg = k2/num_neg #è®¡ç®—æ‰€æœ‰æ¶ˆæè¯æ±‡å‘é‡çš„å‡å€¼\n",
    "import numpy as np\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ªè¯è¯­çš„æƒ…æ„Ÿå¾—åˆ†\n",
    "emotion_scores = []\n",
    "\n",
    "# éå†æ‰€æœ‰è¯è¯­ï¼Œè®¡ç®—æƒ…æ„Ÿå¾—åˆ†å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "for word in words:\n",
    "    score = emo(word)[1]  # å‡è®¾emoå‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªæƒ…æ„Ÿå¾—åˆ†\n",
    "    emotion_scores.append(score)\n",
    "\n",
    "# å°†åˆ—è¡¨è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œä»¥ä¾¿äºè®¡ç®—å¹³å‡å€¼\n",
    "emotion_scores_array = np.array(emotion_scores)\n",
    "\n",
    "# è®¡ç®—æƒ…æ„Ÿå¾—åˆ†çš„å¹³å‡å€¼\n",
    "average_emotion_score = np.mean(emotion_scores_array)\n",
    "\n",
    "# æ‰“å°å¹³å‡æƒ…æ„Ÿå¾—åˆ†\n",
    "print(\"Average Emotion Score:\", average_emotion_score)\n",
    "#ä¸€ä¸ªç”¨äºè®¡ç®—è¯è¯­æƒ…æ„Ÿå¾—åˆ†çš„å‡½æ•°\n",
    "def emo(w):\n",
    "    set_seed = 888\n",
    "    word = model.wv[w]\n",
    "    direction1 = k_pos \n",
    "    direction2 = -k_neg \n",
    "    dian1 = np.dot(word,direction1) #è¯è¯­åœ¨ç§¯æ\n",
    "    dian2 = np.dot(word,direction2) #æ¶ˆæè¿™ä¸ªç»´åº¦ä¸Šçš„æŠ•å½±\n",
    "    sim1_result = dian1/(np.linalg.norm(word)*np.linalg.norm(direction1))\n",
    "    sim2_result = dian2/(np.linalg.norm(word)*np.linalg.norm(direction2))\n",
    "    result1 = \"ã€\" + str(w) + \"ã€‘\" + \"ç§¯ææƒ…æ„Ÿå¾—åˆ†ä¸ºï¼š\" + str(sim1_result) \n",
    "    result2 = \"ã€\" + str(w) + \"ã€‘\" + \"æ¶ˆææƒ…æ„Ÿå¾—åˆ†ä¸ºï¼š\" + str(sim2_result) \n",
    "    return result1,result2,sim1_result,sim2_result\n",
    "print(str(emo(\"èƒœåˆ©\")[0])+ \"\\n\" + str(emo(\"èƒœåˆ©\")[1]))\n",
    "print(str(emo(\"ç–«æƒ…\")[0])+ \"\\n\" + str(emo(\"ç–«æƒ…\")[1]))\n",
    "print(str(emo(\"ç¾å›½\")[0])+ \"\\n\" + str(emo(\"ç¾å›½\")[1]))\n",
    "print(str(emo(\"ç¾å›½ä½¬\")[0])+ \"\\n\" + str(emo(\"ç¾å›½ä½¬\")[1]))\n",
    "print(str(emo(\"åŒ»ç”Ÿ\")[0])+ \"\\n\" + str(emo(\"åŒ»ç”Ÿ\")[1]))\n",
    "print(str(emo(\"æ­»è€…\")[0])+ \"\\n\" + str(emo(\"æ­»è€…\")[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

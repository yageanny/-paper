{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                    原文\n",
       "0   上诉人渤海银行股份有限公司郑州分行（以下简称渤海银行郑州分行）因与被上诉人张允春、左党杰保证...\n",
       "1   重庆市第二中级人民法院\\n民 事 判 决 书\\n（2016）渝02民终2793号\\n上诉人（...\n",
       "2   广东省东莞市中级人民法院\\n民 事 判 决 书\\n(2021)粤19民终891号\\n上诉人（...\n",
       "3   安徽省合肥市中级人民法院\\n民 事 判 决 书\\n(2018)皖01民终9587号\\n上诉人...\n",
       "4   湖北省武汉市中级人民法院\\n民 事 判 决 书\\n（2020）鄂01民终3588号\\n上诉人...\n",
       "..                                                ...\n",
       "65  福建省泉州市中级人民法院\\n民 事 裁 定 书\\n（2020）闽05民终3055号\\n上诉人...\n",
       "66  北京市第二中级人民法院\\n民 事 判 决 书\\n（2019）京02民终5844号\\n上诉人（...\n",
       "67  河南省郑州市中级人民法院\\n民 事 判 决 书\\n（2019）豫01民终19941号\\n上诉...\n",
       "68  吉林省长春市中级人民法院\\n民 事 判 决 书\\n（2016）吉01民终2206号\\n上诉人...\n",
       "69   \\n \\n广西壮族自治区贺州市中级人民法院\\n民 事 判 决 书\\n（2019）桂11民终...\n",
       "\n",
       "[70 rows x 1 columns]>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取Stata的dta文件\n",
    "df = pd.read_excel('D://2023年寒假//司法大数据//原文.xlsx')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  标题  \\\n",
      "0        临汾市人民政府关于表彰2011年度环境保护先进单...   \n",
      "1   临汾市人民政府关于表彰2012年全市环卫工作先进单位和先进...   \n",
      "2  临环发〔2018〕75号关于调整临汾市环境保护局扫黑除恶专项...   \n",
      "3        大气污染防治强化督查第十九轮次执法人员 临时执法证公示   \n",
      "4    临汾环保局迎来新局长:深陷环境数据造假风波,两年三易主官...   \n",
      "\n",
      "                                                标题链接           时间  \\\n",
      "0  http://www.baidu.com/link?url=ddXSW6Q27zhzjJGq...          NaN   \n",
      "1  http://www.baidu.com/link?url=ddXSW6Q27zhzjJGq...  2012年10月19日   \n",
      "2  http://www.baidu.com/link?url=C5jn7ivO-IOasyOP...   2018年9月15日   \n",
      "3  http://www.baidu.com/link?url=vlNHL_ovqHwZIuZe...  2017年12月19日   \n",
      "4  http://www.baidu.com/link?url=ddXSW6Q27zhzjJGq...    2018年8月8日   \n",
      "\n",
      "     ccolorgray                                                 内容      文本  \\\n",
      "0   临汾市人民政府门户网站  2011年,全市上下按照市委、市政府统一部署,紧密结合我市工作实际,围绕年初确定的工作目标任...  播报暂停   \n",
      "1   临汾市人民政府门户网站  各县、市、区人民政府,临汾、侯马经济开发区管委会,壶口风景区管委会,市直有关部门: 一年来,...  播报暂停   \n",
      "2      临汾市生态环境局  组  长:李永芳  党组书记、局长 副组长:邓明生  党组成员、副局长         徐红...  播报暂停   \n",
      "3  中华人民共和国生态环境部  副局长 中队长 中队长 中队长 科员 副局长 副大队长 中队长 大队长 科员 中队长 中队长...     NaN   \n",
      "4          澎湃新闻  2016年8月30日,时年52岁的郭波(1964.11)出任临汾市环保局局长,郭波此前曾任临...  播报暂停   \n",
      "\n",
      "                                                 文本1  \\\n",
      "0  2011年,全市上下按照市委、市政府统一部署,紧密结合我市工作实际,围绕年初确定的工作目标任...   \n",
      "1                                        2012年10月19日   \n",
      "2                                         2018年9月15日   \n",
      "3                                        2017年12月19日   \n",
      "4                                          2018年8月8日   \n",
      "\n",
      "                                            combined  \n",
      "0  播报暂停 2011年,全市上下按照市委、市政府统一部署,紧密结合我市工作实际,围绕年初确...  \n",
      "1  播报暂停 各县、市、区人民政府,临汾、侯马经济开发区管委会,壶口风景区管委会,市直有关部...  \n",
      "2  播报暂停 组  长:李永芳  党组书记、局长 副组长:邓明生  党组成员、副局长    ...  \n",
      "3  nan 副局长 中队长 中队长 中队长 科员 副局长 副大队长 中队长 大队长 科员 中队长...  \n",
      "4  播报暂停 2016年8月30日,时年52岁的郭波(1964.11)出任临汾市环保局局长,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 读取Stata的dta文件\n",
    "df = pd.read_excel('D://2023年寒假//论文分析//爬虫/临汾环保局局长_百度搜索.xlsx')\n",
    "df['combined'] = df['文本'].astype(str) + \" \"  + df['内容'].astype(str)  + \" \" + df['文本1'].astype(str) #+ \" \" + df['文本2'].astype(str)\n",
    "#df['combined'] = df['文本'].astype(str) \n",
    "#df['combined'] = df['文本'].astype(str) + \" \"  + df['内容'].astype(str)  \n",
    "# Assuming '文本' and '内容' are the names of the columns you want to combine\n",
    "#df['combined'] = df['文本'].astype(str) + \" \" + df['内容'].astype(str)\n",
    "\n",
    "# Show the first few rows of the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              combined 局长姓名\n",
      "0    播报暂停 2011年,全市上下按照市委、市政府统一部署,紧密结合我市工作实际,围绕年初确...     \n",
      "1    播报暂停 各县、市、区人民政府,临汾、侯马经济开发区管委会,壶口风景区管委会,市直有关部...     \n",
      "2    播报暂停 组  长:李永芳  党组书记、局长 副组长:邓明生  党组成员、副局长    ...     \n",
      "3    nan 副局长 中队长 中队长 中队长 科员 副局长 副大队长 中队长 大队长 科员 中队长...     \n",
      "4    播报暂停 2016年8月30日,时年52岁的郭波(1964.11)出任临汾市环保局局长,...  ,郭波\n",
      "..                                                 ...  ...\n",
      "179  播报暂停 7月24日,呼伦贝尔专门召开市委常委会,迅速安排市环保局和公安局组成专项调查组...     \n",
      "180  10、吕梁市公安局“扫黑办” 举报电话:0358-8310110 0358-8310226 ...     \n",
      "181  播报暂停 进入2017年,环保部1月19日对山西临汾市政府主要负责人进行了约谈,原因是“...     \n",
      "182  nan 栗纲(局党组书记、局长):对市局的党风廉政建设和行政监察工作负总责。 毕研华(局党组...     \n",
      "183  nan 换届在即,市长突然病危,这一骤然事变打乱了省委和市委既定的人事安排计划,全市方方面面...     \n",
      "\n",
      "[184 rows x 2 columns]\n",
      "                                              combined 局长姓名\n",
      "0    播报暂停 2011年,全市上下按照市委、市政府统一部署,紧密结合我市工作实际,围绕年初确...     \n",
      "1    播报暂停 各县、市、区人民政府,临汾、侯马经济开发区管委会,壶口风景区管委会,市直有关部...     \n",
      "2    播报暂停 组  长:李永芳  党组书记、局长 副组长:邓明生  党组成员、副局长    ...     \n",
      "3    nan 副局长 中队长 中队长 中队长 科员 副局长 副大队长 中队长 大队长 科员 中队长...     \n",
      "4    播报暂停 2016年8月30日,时年52岁的郭波(1964.11)出任临汾市环保局局长,...  ,郭波\n",
      "..                                                 ...  ...\n",
      "179  播报暂停 7月24日,呼伦贝尔专门召开市委常委会,迅速安排市环保局和公安局组成专项调查组...     \n",
      "180  10、吕梁市公安局“扫黑办” 举报电话:0358-8310110 0358-8310226 ...     \n",
      "181  播报暂停 进入2017年,环保部1月19日对山西临汾市政府主要负责人进行了约谈,原因是“...     \n",
      "182  nan 栗纲(局党组书记、局长):对市局的党风廉政建设和行政监察工作负总责。 毕研华(局党组...     \n",
      "183  nan 换届在即,市长突然病危,这一骤然事变打乱了省委和市委既定的人事安排计划,全市方方面面...     \n",
      "\n",
      "[184 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-0dbac524a0ef>:14: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.7' currently installed).\n",
      "  df.to_excel('D://2023年寒假//论文分析//爬虫/“临汾”“环保局局长”name_百度搜索.xlsx', index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "#df['局长姓名'] = df['combined'].apply(lambda x: re.search(r\"(?:包头市环保局局长|包头环保局局长)(\\S{2,3})\", x).group(1) if re.search(r\"(?:包头市环保局局长|包头环保局局长)(\\S{2,3})\", x) else '')\n",
    "\n",
    "pattern = r\"临汾市?环保局局长(\\S{2,3})|(?<=\\()(\\S{2,3})(?=市环保局局长\\))\"\n",
    "\n",
    "# 应用正则表达式并提取匹配的姓名\n",
    "df['局长姓名'] = df['combined'].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else '')\n",
    "\n",
    "# 输出查看结果\n",
    "print(df[['combined', '局长姓名']])\n",
    "print(df[['combined', '局长姓名']])\n",
    "#df.to_csv('.csv')\n",
    "df.to_excel('D://2023年寒假//论文分析//爬虫/“临汾”“环保局局长”name_百度搜索.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     ：1.《张允春、左党杰涉诉信息》表及相关裁判文书，据以证明二人在2018、2019年间为建设...\n",
      "1     材料并不能证明陈登贵已获得了本案诉请获得的服务单据。其次，陈登贵之夫张净作为刑事案件的被告人...\n",
      "2     材料，陈美云对相关证据的真实性、合法性、关联性均予以确认，案涉合同也明确有犹豫期的约定，陈美...\n",
      "3     ：一、国元证券公司投资顾问公示、微信截图、来往港澳通行证截图。证据内容：陈佳担任国元证券重庆...\n",
      "4     七（理财产品客户协议书、陈义丰身份证复印件、个人客户风险评估问卷）认定错误，而导致事实认定错...\n",
      "                            ...                        \n",
      "65    起诉要求吴移山承担连带保证责任，其认为本案案由应为保证合同纠纷，而一审法院认为本案案由系金融...\n",
      "66    证明已依据审慎性原则对李昭进行了提示和告知并最终得到李昭的确认。因此，工行德胜科技园支行在履...\n",
      "67                                     证明其损失,故一审法院不予支持。\n",
      "68    的事实与原审判决认定的事实基本一致。另查明，卡号为XXX号的信用卡使用期限至2016年6月。...\n",
      "69    向本院提交。\\n综合诉辩意见，双方当事人对一审查明的事实没有异议，本院予以确认。上诉人荣盛公...\n",
      "Name: 二审法院, Length: 70, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_facts(text):\n",
    "    pattern = r'(二审.*?证据|本院二审提交新证据。|本院审理后认为，|二审.*?提交.*?印件|本院二审.*?提供|被上诉人.*?书面答辩意见。|在法定期限内未提交答辩意见|.*?未提交答辩意见|均未明确规定与本案相类似|本院认为：郑桁在一审时|二审经审理查明，|本案经本院二审审理查明|除原审查明事实部分的内容，本院二审说明如下事实：|本案二审.*?提交.*?证据)(.*?)(依照《中华人民共和国|依照《最高人民法院|依据《中华人民共和国民事)'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(2).strip()  # 返回第二个捕获组的内容，即关键词之后到“依照《中华人民共和国民法”之前的所有内容\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['二审法院'] = df['原文'].apply(extract_facts)\n",
    "\n",
    "print(df['二审法院'])\n",
    "#df.to_csv('原文.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "<>:35: DeprecationWarning: invalid escape sequence \\p\n",
      "<>:35: DeprecationWarning: invalid escape sequence \\p\n",
      "<ipython-input-266-ee17131b61e1>:35: DeprecationWarning: invalid escape sequence \\p\n",
      "  stopwords = get_stop_words('D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      涉诉 信息 表及 相关 裁判 文书 证明 二人 2018 2019 年间 建设 集团 四笔...\n",
      "1     材料 不能 证明 获得 本案 诉请 获得 服务 单据 之夫 刑事案件 被告人 均 未能 从院...\n",
      "2     材料 相关 证据 真实性 合法性 关联性 均 予以 确认 案涉 合同 明确 犹豫 期 约定 ...\n",
      "3     国元 证券公司 投资 顾问 公示 微信 截图 来往 港澳 通行证 截图 证据 内容 任国元 ...\n",
      "4     七  理财产品 客户 协议书 身份证 复印件 个人 客户 风险 评估 问卷  认定 错误 导...\n",
      "                            ...                        \n",
      "65    起诉 要求 承担 连带保证 责任 认为 本案 案由 应为 保证 合同纠纷 一审 法院 认为 ...\n",
      "66    证明 审慎 性 原则 对行 提示 告知 最终 得到 确认 工行 德胜 科技园 支行 履行 风...\n",
      "67                                  证明 损失 , 一审 法院 不予 支持\n",
      "68    事实 原审 判决 认定 事实 基本一致 查明 卡号 XXX 号 信用卡 使用 期限 2016...\n",
      "69    本院 提交 综合 诉辩 意见 双方 当事人 审查 明 事实 没有 异议 本院 予以 确认 上...\n",
      "Name: 二审法院3, Length: 70, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "# 自定义词汇列表\n",
    "custom_words = [\"损失赔偿\", \"免责事由\", \"告知说明\", \"举证责任\", \"法律适用\", \"适当性\", \"责任主体\", \"九民纪要\", \"证券\", \"虚假','陈述\", \"侵权\", \"交易\", \"代理\", \"财产\", \"损害\", \"赔偿\", \"金融\", \"委托\", \"理财\", \"融资\", \"融券\"]\n",
    "\n",
    "# 将自定义词汇添加到结巴分词的词典中\n",
    "for word in custom_words:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "# 加载停用词\n",
    "def get_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f])\n",
    "    return stopwords\n",
    "\n",
    "# 去除特定的中文姓名\n",
    "def remove_specific_chinese_names(text, common_surnames):\n",
    "    pattern = '|'.join([f'{surname}[\\u4e00-\\u9fa5]{{1,2}}' for surname in common_surnames])\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# 去除特定的地点名称\n",
    "def remove_specific_places(text):\n",
    "    pattern = r'\\w*(市|州|门)'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# 结巴分词并去除停用词\n",
    "def jieba_cut_remove_stopwords(text, stopwords):\n",
    "    words = jieba.lcut(text)\n",
    "    return ' '.join([word for word in words if word not in stopwords and word.strip()])\n",
    "def remove_punctuation(text):\n",
    "    punctuation = r\"\"\"！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\"\"\n",
    "    return re.sub(r\"[%s]+\" % punctuation, \"\", text)\n",
    "# 加载停用词\n",
    "stopwords = get_stop_words('D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt')\n",
    "\n",
    "# 定义常见的姓氏\n",
    "common_surnames = ['王', '左', '汪', '吕', '肖', '汤', '熊', '陶', '李', '张', '梁', '刘', '阚', '康', '陈', '杨', '赵', '黄', '周', '吴', '徐', '孙', '胡', '朱', '高', '林', '何', '郭', '马', '罗']\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'二审法院'的列\n",
    "df['二审法院2'] = df['二审法院'].apply(lambda x: remove_specific_places(x))\n",
    "df['二审法院2.1'] = df['二审法院2'].apply(lambda x: remove_specific_chinese_names(x, common_surnames))\n",
    "df['二审法院2.2'] = df['二审法院2.1'].apply(lambda x: jieba_cut_remove_stopwords(x, stopwords))\n",
    "df['二审法院3'] = df['二审法院2.2'].apply(lambda x: remove_punctuation(x))\n",
    "# 查看处理后的结果\n",
    "print(df['二审法院3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 去掉空字符\n",
    "def delete_r_n(line):\n",
    "    return line.replace('\\r', '').replace('\\n', '').strip()\n",
    "\n",
    "# 加载停用词\n",
    "def get_stop_words(stop_words_dir):\n",
    "    stop_words = []\n",
    "\n",
    "    with open(stop_words_dir, 'r', encoding='utf-8') as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = delete_r_n(line)\n",
    "            stop_words.append(line)\n",
    "\n",
    "    stop_words = set(stop_words)\n",
    "    return stop_words\n",
    "\n",
    "# 加载停用词,stopwords为停用词列表\n",
    "stopwords = get_stop_words(r'停用词表.txt')\n",
    "\n",
    "#stopwords = get_stop_words(r'D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt')\n",
    "\n",
    "def jieba_cut_remove_stopwords(text):\n",
    "    \"\"\"结巴分词，并去除停用词\n",
    "    \"\"\"\n",
    "    words = jieba.lcut(text)\n",
    "    results = []\n",
    "    for word in words:\n",
    "        # 去除停用词和空元素\n",
    "        if word not in stopwords and word.strip():\n",
    "            results.append(word)\n",
    "    return ' '.join(results)\n",
    "\n",
    "import jieba\n",
    "\n",
    "# 加载自定义词典\n",
    "jieba.load_userdict(\"custom.txt\")\n",
    "\n",
    "# 定义一个分词函数\n",
    "def jieba_cut(text):\n",
    "    return ' '.join(jieba.cut(text))\n",
    "\n",
    "# 应用分词函数到DataFrame的列\n",
    "df['二审法院2.2.2'] = df['二审法院'].apply(jieba_cut)\n",
    "\n",
    "# 输出分词结果的前几行\n",
    "print(df['二审法院2.2.2'].head())\n",
    "\n",
    "df['二审法院3.1'] = df[\"二审法院3\"].apply(lambda x:jieba_cut_remove_stopwords(str(x)))\n",
    "\n",
    "df['二审法院3.1'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('原文.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词库，用于对中文文本进行分词处理\n",
    "import jieba\n",
    "# 数据处理和分析库，用于处理和分析数据\n",
    "import pandas as pd\n",
    "# Gensim库中的corpora和models模块，用于主题建模和文本处理\n",
    "from gensim import corpora, models\n",
    "# Scikit-learn库中的CountVectorizer，用于将文本转换为词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 绘图库，用于数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "# 忽略警告信息\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "# 数值计算库，用于数值运算\n",
    "import numpy as np\n",
    "# pyLDAvis库中的gensim_models模块，用于可视化LDA主题模型\n",
    "import pyLDAvis.gensim_models \n",
    "# Gensim库中的CoherenceModel、LdaModel和Dictionary，用于计算主题一致性和LDA模型\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# 中文显示支持，设置字体为SimHei\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# BERTopic库，用于BERT主题建模\n",
    "#from bertopic import BERTopic\n",
    "# Spacy库，用于自然语言处理，加载中文模型'zh_core_web_sm'\n",
    "import spacy\n",
    "nlp_model = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    涉诉 信息 表及 相关 裁判 文书 证明 二人 20182019 年间 建设 集团 四笔 贷...\n",
      "1    材料 证明 本案 诉请 服务 单据 之夫 刑事案件 被告人 未能 法院 刑事案件 档案资料 ...\n",
      "2    材料 相关 证据 真实性 合法性 关联性 予以 确认 案涉 合同 犹豫 期 约定 收悉 合同...\n",
      "3    一国元 证券公司 投资 顾问 公示 微信 截图 来往 港澳 通行证 截图 证据 内容 证券 ...\n",
      "4    理财产品 客户 协议书 身份证 复印件 客户 风险 评估 问卷 认定 错误 导致 事实 认定...\n",
      "Name: 二审法院_processed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 自定义词汇列表\n",
    "custom_words = [\"损失赔偿\", \"免责事由\", \"告知说明\", \"举证责任\", \"法律适用\", \"适当性\", \"责任主体\", \"九民纪要\", \"证券\", \"虚假\",\"陈述\", \"侵权\", \"交易\", \"代理\", \"财产\", \"损害\", \"赔偿\", \"金融\", \"委托\", \"理财\", \"融资\", \"融券\"]\n",
    "\n",
    "# 将自定义词汇添加到结巴分词的词典中\n",
    "for word in custom_words:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "# 加载自定义词典\n",
    "#jieba.load_userdict(\"custom.txt\")\n",
    "# 加载停用词\n",
    "def get_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f])\n",
    "    return stopwords\n",
    "\n",
    "# 去除特定的中文姓名\n",
    "def remove_specific_chinese_names(text, common_surnames):\n",
    "    pattern = '|'.join([f'{surname}[\\u4e00-\\u9fa5]{{1,2}}' for surname in common_surnames])\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# 去除特定的地点名称\n",
    "def remove_specific_places(text):\n",
    "    pattern = r'\\w*(市|州|门)'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# 去除标点符号\n",
    "def remove_punctuation(text):\n",
    "    punctuation = r\"\"\"！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\"\"\n",
    "    return re.sub(r\"[%s]+\" % punctuation, \"\", text)\n",
    "\n",
    "# 结巴分词并去除停用词\n",
    "def jieba_cut_remove_stopwords(text, stopwords):\n",
    "    words = jieba.lcut(text)\n",
    "    return ' '.join([word for word in words if word not in stopwords and word.strip()])\n",
    "\n",
    "# 加载停用词\n",
    "stopwords = get_stop_words(\"停用词表.txt\")\n",
    "\n",
    "# 定义常见的姓氏\n",
    "common_surnames = ['王','阚','汪', '李','任', '张', '刘', '陈', '杨', '赵', '黄', '周', '吴', '徐', '孙', '胡', '朱', '高', '林', '何', '郭', '马', '罗']\n",
    "\n",
    "# 应用所有文本处理步骤\n",
    "def process_text(text, stopwords, common_surnames):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_specific_places(text)\n",
    "    text = remove_specific_chinese_names(text, common_surnames)\n",
    "    text = jieba_cut_remove_stopwords(text, stopwords)\n",
    "    return text\n",
    "\n",
    "# 应用函数到DataFrame的列\n",
    "df['二审法院_processed'] = df['二审法院'].apply(lambda x: process_text(x, stopwords, common_surnames))\n",
    "\n",
    "# 输出处理后的结果\n",
    "print(df['二审法院_processed'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 提取信息  上诉人是否个体\n",
      "0     {'上诉人': '', '被上诉人': '', '原审被告': '', '原审原告': ''}        0\n",
      "1   {'上诉人': '陈登贵，女，汉族，生于1945年7月27日，住重庆市南岸区', '被上诉人...        1\n",
      "2   {'上诉人': '陈美云，女，1987年12月26日出生，汉族，住广东省电白县×××××××...        1\n",
      "3   {'上诉人': '陈行，男，1987年1月27日出生，汉族，住浙江省温州市瓯海区', '被上...        1\n",
      "4   {'上诉人': '中国建设银行股份有限公司武汉常福新城支行，住所地武汉市蔡甸区奓山街白鹤泉西...        0\n",
      "..                                                ...      ...\n",
      "65  {'上诉人': '郑桁，男，1953年10月18日出生，香港特别行政区居民，现住福建省泉州市...        1\n",
      "66  {'上诉人': '中国工商银行股份有限公司北京德胜科技园支行，住北京市西城区德胜门外大街13...        0\n",
      "67  {'上诉人': '中国农业银行股份有限公司荥阳市支行，住所地荥阳市索河路与京城路交叉口', ...        0\n",
      "68  {'上诉人': '中信银行股份有限公司长春分行', '被上诉人': '李声，男，1955年5...        0\n",
      "69  {'上诉人': '贺州市荣盛家电有限责任公司，住所地：贺州市八达西路北侧鑫海酒店对面商铺',...        0\n",
      "\n",
      "[70 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 定义正则表达式\n",
    "pattern = {\n",
    "    '上诉人':  r'上诉人.*?：(.*?)。',\n",
    "    '被上诉人': r'被上诉人.*?：(.*?)。',\n",
    "    '原审被告': r'原审被告.*?：(.*?)。',\n",
    "    '原审原告': r'原审原告.*?：(.*?)。'\n",
    "}\n",
    "\n",
    "# 提取信息\n",
    "def extract_info(text):\n",
    "    info = {key: re.search(pattern[key], text).group(1) if re.search(pattern[key], text) else '' for key in pattern}\n",
    "    return info\n",
    "\n",
    "# 判断上诉人是否为个体\n",
    "def is_individual(appellant):\n",
    "    return 1 if any(gender in appellant for gender in ['男', '女']) else 0\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['提取信息'] = df['原文'].apply(extract_info)\n",
    "df['上诉人是否个体'] = df['提取信息'].apply(lambda x: is_individual(x['上诉人']))\n",
    "\n",
    "print(df[['提取信息', '上诉人是否个体']])\n",
    "df.to_csv(\"原文2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    审判结果  上诉人是否个体\n",
      "0      1        0\n",
      "1      1        1\n",
      "2      1        1\n",
      "3      1        1\n",
      "4      1        0\n",
      "..   ...      ...\n",
      "65     1        1\n",
      "66     0        0\n",
      "67     1        0\n",
      "68     0        0\n",
      "69     0        0\n",
      "\n",
      "[70 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def judge_result(text):\n",
    "    return 1 if \"维持原判\" in text else 0\n",
    "\n",
    "df['审判结果'] = df['原文'].apply(judge_result)\n",
    "print(df[['审判结果', '上诉人是否个体']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  原文  \\\n",
      "0  上诉人渤海银行股份有限公司郑州分行（以下简称渤海银行郑州分行）因与被上诉人张允春、左党杰保证...   \n",
      "1  重庆市第二中级人民法院\\n民 事 判 决 书\\n（2016）渝02民终2793号\\n上诉人（...   \n",
      "2  广东省东莞市中级人民法院\\n民 事 判 决 书\\n(2021)粤19民终891号\\n上诉人（...   \n",
      "3  安徽省合肥市中级人民法院\\n民 事 判 决 书\\n(2018)皖01民终9587号\\n上诉人...   \n",
      "4  湖北省武汉市中级人民法院\\n民 事 判 决 书\\n（2020）鄂01民终3588号\\n上诉人...   \n",
      "\n",
      "                                                二审法院  \\\n",
      "0  ：1.《张允春、左党杰涉诉信息》表及相关裁判文书，据以证明二人在2018、2019年间为建设...   \n",
      "1  材料并不能证明陈登贵已获得了本案诉请获得的服务单据。其次，陈登贵之夫张净作为刑事案件的被告人...   \n",
      "2  材料，陈美云对相关证据的真实性、合法性、关联性均予以确认，案涉合同也明确有犹豫期的约定，陈美...   \n",
      "3  ：一、国元证券公司投资顾问公示、微信截图、来往港澳通行证截图。证据内容：陈佳担任国元证券重庆...   \n",
      "4  七（理财产品客户协议书、陈义丰身份证复印件、个人客户风险评估问卷）认定错误，而导致事实认定错...   \n",
      "\n",
      "                                      二审法院_processed  \\\n",
      "0  涉诉 信息 表及 相关 裁判 文书 证明 二人 20182019 年间 建设 集团 四笔 贷...   \n",
      "1  材料 证明 本案 诉请 服务 单据 之夫 刑事案件 被告人 未能 法院 刑事案件 档案资料 ...   \n",
      "2  材料 相关 证据 真实性 合法性 关联性 予以 确认 案涉 合同 犹豫 期 约定 收悉 合同...   \n",
      "3  一国元 证券公司 投资 顾问 公示 微信 截图 来往 港澳 通行证 截图 证据 内容 证券 ...   \n",
      "4  理财产品 客户 协议书 身份证 复印件 客户 风险 评估 问卷 认定 错误 导致 事实 认定...   \n",
      "\n",
      "                                                提取信息  上诉人是否个体  审判结果  有利于消费者  \n",
      "0    {'上诉人': '', '被上诉人': '', '原审被告': '', '原审原告': ''}        0     1       1  \n",
      "1  {'上诉人': '陈登贵，女，汉族，生于1945年7月27日，住重庆市南岸区', '被上诉人...        1     1       0  \n",
      "2  {'上诉人': '陈美云，女，1987年12月26日出生，汉族，住广东省电白县×××××××...        1     1       0  \n",
      "3  {'上诉人': '陈行，男，1987年1月27日出生，汉族，住浙江省温州市瓯海区', '被上...        1     1       0  \n",
      "4  {'上诉人': '中国建设银行股份有限公司武汉常福新城支行，住所地武汉市蔡甸区奓山街白鹤泉西...        0     1       1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设上诉人是否个体的列为'上诉人是否个体'，审判结果的列为'审判结果'\n",
    "# 创建一个函数，根据条件返回新列的值\n",
    "def create_consumer_favorable(row):\n",
    "    if row['上诉人是否个体'] == 1 and row['审判结果'] == 0:\n",
    "        return 1\n",
    "    elif row['审判结果'] == 1 and row['上诉人是否个体'] == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 使用apply函数将新列应用到DataFrame\n",
    "df['有利于消费者'] = df.apply(create_consumer_favorable, axis=1)\n",
    "\n",
    "# 打印带有新列的DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设文本列的名称为'未分词文本'\n",
    "# 创建一个函数，用于检查文本中是否包含指定词汇\n",
    "def contains_keyword(text):\n",
    "    keyword = \"九民纪要\"\n",
    "    if keyword in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 使用apply函数将新列应用到DataFrame\n",
    "df['包含九民纪要'] = df['原文'].apply(contains_keyword)\n",
    "\n",
    "# 打印带有新列的DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  原文  \\\n",
      "0  上诉人渤海银行股份有限公司郑州分行（以下简称渤海银行郑州分行）因与被上诉人张允春、左党杰保证...   \n",
      "1  重庆市第二中级人民法院\\n民 事 判 决 书\\n（2016）渝02民终2793号\\n上诉人（...   \n",
      "2  广东省东莞市中级人民法院\\n民 事 判 决 书\\n(2021)粤19民终891号\\n上诉人（...   \n",
      "3  安徽省合肥市中级人民法院\\n民 事 判 决 书\\n(2018)皖01民终9587号\\n上诉人...   \n",
      "4  湖北省武汉市中级人民法院\\n民 事 判 决 书\\n（2020）鄂01民终3588号\\n上诉人...   \n",
      "\n",
      "                                                二审法院  \\\n",
      "0  ：1.《张允春、左党杰涉诉信息》表及相关裁判文书，据以证明二人在2018、2019年间为建设...   \n",
      "1  材料并不能证明陈登贵已获得了本案诉请获得的服务单据。其次，陈登贵之夫张净作为刑事案件的被告人...   \n",
      "2  材料，陈美云对相关证据的真实性、合法性、关联性均予以确认，案涉合同也明确有犹豫期的约定，陈美...   \n",
      "3  ：一、国元证券公司投资顾问公示、微信截图、来往港澳通行证截图。证据内容：陈佳担任国元证券重庆...   \n",
      "4  七（理财产品客户协议书、陈义丰身份证复印件、个人客户风险评估问卷）认定错误，而导致事实认定错...   \n",
      "\n",
      "                                      二审法院_processed  \\\n",
      "0  涉诉 信息 表及 相关 裁判 文书 证明 二人 20182019 年间 建设 集团 四笔 贷...   \n",
      "1  材料 证明 本案 诉请 服务 单据 之夫 刑事案件 被告人 未能 法院 刑事案件 档案资料 ...   \n",
      "2  材料 相关 证据 真实性 合法性 关联性 予以 确认 案涉 合同 犹豫 期 约定 收悉 合同...   \n",
      "3  一国元 证券公司 投资 顾问 公示 微信 截图 来往 港澳 通行证 截图 证据 内容 证券 ...   \n",
      "4  理财产品 客户 协议书 身份证 复印件 客户 风险 评估 问卷 认定 错误 导致 事实 认定...   \n",
      "\n",
      "                                                提取信息  上诉人是否个体  审判结果  有利于消费者  \\\n",
      "0    {'上诉人': '', '被上诉人': '', '原审被告': '', '原审原告': ''}        0     1       1   \n",
      "1  {'上诉人': '陈登贵，女，汉族，生于1945年7月27日，住重庆市南岸区', '被上诉人...        1     1       0   \n",
      "2  {'上诉人': '陈美云，女，1987年12月26日出生，汉族，住广东省电白县×××××××...        1     1       0   \n",
      "3  {'上诉人': '陈行，男，1987年1月27日出生，汉族，住浙江省温州市瓯海区', '被上...        1     1       0   \n",
      "4  {'上诉人': '中国建设银行股份有限公司武汉常福新城支行，住所地武汉市蔡甸区奓山街白鹤泉西...        0     1       1   \n",
      "\n",
      "   包含一审程序违法  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# 假设文本列的名称为'未分词文本'\n",
    "# 创建一个函数，用于检查文本中是否包含指定词汇\n",
    "def contains_keyword(text):\n",
    "    keyword = \"一审程序违法\"\n",
    "    if keyword in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 使用apply函数将新列应用到DataFrame\n",
    "df['包含一审程序违法'] = df['二审法院'].apply(contains_keyword)\n",
    "\n",
    "# 打印带有新列的DataFrame\n",
    "print(df.head())\n",
    "\n",
    "#66、4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   原文  \\\n",
      "0   上诉人渤海银行股份有限公司郑州分行（以下简称渤海银行郑州分行）因与被上诉人张允春、左党杰保证...   \n",
      "1   重庆市第二中级人民法院\\n民 事 判 决 书\\n（2016）渝02民终2793号\\n上诉人（...   \n",
      "2   广东省东莞市中级人民法院\\n民 事 判 决 书\\n(2021)粤19民终891号\\n上诉人（...   \n",
      "3   安徽省合肥市中级人民法院\\n民 事 判 决 书\\n(2018)皖01民终9587号\\n上诉人...   \n",
      "4   湖北省武汉市中级人民法院\\n民 事 判 决 书\\n（2020）鄂01民终3588号\\n上诉人...   \n",
      "..                                                ...   \n",
      "65  福建省泉州市中级人民法院\\n民 事 裁 定 书\\n（2020）闽05民终3055号\\n上诉人...   \n",
      "66  北京市第二中级人民法院\\n民 事 判 决 书\\n（2019）京02民终5844号\\n上诉人（...   \n",
      "67  河南省郑州市中级人民法院\\n民 事 判 决 书\\n（2019）豫01民终19941号\\n上诉...   \n",
      "68  吉林省长春市中级人民法院\\n民 事 判 决 书\\n（2016）吉01民终2206号\\n上诉人...   \n",
      "69   \\n \\n广西壮族自治区贺州市中级人民法院\\n民 事 判 决 书\\n（2019）桂11民终...   \n",
      "\n",
      "                                                 二审法院  \\\n",
      "0   ：1.《张允春、左党杰涉诉信息》表及相关裁判文书，据以证明二人在2018、2019年间为建设...   \n",
      "1   材料并不能证明陈登贵已获得了本案诉请获得的服务单据。其次，陈登贵之夫张净作为刑事案件的被告人...   \n",
      "2   材料，陈美云对相关证据的真实性、合法性、关联性均予以确认，案涉合同也明确有犹豫期的约定，陈美...   \n",
      "3   ：一、国元证券公司投资顾问公示、微信截图、来往港澳通行证截图。证据内容：陈佳担任国元证券重庆...   \n",
      "4   七（理财产品客户协议书、陈义丰身份证复印件、个人客户风险评估问卷）认定错误，而导致事实认定错...   \n",
      "..                                                ...   \n",
      "65  起诉要求吴移山承担连带保证责任，其认为本案案由应为保证合同纠纷，而一审法院认为本案案由系金融...   \n",
      "66  证明已依据审慎性原则对李昭进行了提示和告知并最终得到李昭的确认。因此，工行德胜科技园支行在履...   \n",
      "67                                   证明其损失,故一审法院不予支持。   \n",
      "68  的事实与原审判决认定的事实基本一致。另查明，卡号为XXX号的信用卡使用期限至2016年6月。...   \n",
      "69  向本院提交。\\n综合诉辩意见，双方当事人对一审查明的事实没有异议，本院予以确认。上诉人荣盛公...   \n",
      "\n",
      "                                       二审法院_processed  \\\n",
      "0   涉诉 信息 表及 相关 裁判 文书 证明 二人 20182019 年间 建设 集团 四笔 贷...   \n",
      "1   材料 证明 本案 诉请 服务 单据 之夫 刑事案件 被告人 未能 法院 刑事案件 档案资料 ...   \n",
      "2   材料 相关 证据 真实性 合法性 关联性 予以 确认 案涉 合同 犹豫 期 约定 收悉 合同...   \n",
      "3   一国元 证券公司 投资 顾问 公示 微信 截图 来往 港澳 通行证 截图 证据 内容 证券 ...   \n",
      "4   理财产品 客户 协议书 身份证 复印件 客户 风险 评估 问卷 认定 错误 导致 事实 认定...   \n",
      "..                                                ...   \n",
      "65  起诉 承担 连带保证 责为 本案 案由 应为 保证 合同纠纷 一审 法院 本案 案由 系 金...   \n",
      "66  证明 审慎 性 原则 对行 提示 告知 最终 确认 工行 德胜 科技园 支行 履行 风险 提...   \n",
      "67                                  证明 损失 一审 法院 不予 支持   \n",
      "68  事实 原审 判决 认定 事实 基本一致 查明 卡号 XXX 信用卡 期限 2016 二审 庭...   \n",
      "69  本院 提交 综合 诉辩 意见 当事人 审查 明 事实 异议 本院 予以 确认 荣盛 公司 一...   \n",
      "\n",
      "                                                 提取信息  上诉人是否个体  审判结果  有利于消费者  \\\n",
      "0     {'上诉人': '', '被上诉人': '', '原审被告': '', '原审原告': ''}        0     1       1   \n",
      "1   {'上诉人': '陈登贵，女，汉族，生于1945年7月27日，住重庆市南岸区', '被上诉人...        1     1       0   \n",
      "2   {'上诉人': '陈美云，女，1987年12月26日出生，汉族，住广东省电白县×××××××...        1     1       0   \n",
      "3   {'上诉人': '陈行，男，1987年1月27日出生，汉族，住浙江省温州市瓯海区', '被上...        1     1       0   \n",
      "4   {'上诉人': '中国建设银行股份有限公司武汉常福新城支行，住所地武汉市蔡甸区奓山街白鹤泉西...        0     1       1   \n",
      "..                                                ...      ...   ...     ...   \n",
      "65  {'上诉人': '郑桁，男，1953年10月18日出生，香港特别行政区居民，现住福建省泉州市...        1     1       0   \n",
      "66  {'上诉人': '中国工商银行股份有限公司北京德胜科技园支行，住北京市西城区德胜门外大街13...        0     0       0   \n",
      "67  {'上诉人': '中国农业银行股份有限公司荥阳市支行，住所地荥阳市索河路与京城路交叉口', ...        0     1       1   \n",
      "68  {'上诉人': '中信银行股份有限公司长春分行', '被上诉人': '李声，男，1955年5...        0     0       0   \n",
      "69  {'上诉人': '贺州市荣盛家电有限责任公司，住所地：贺州市八达西路北侧鑫海酒店对面商铺',...        0     0       0   \n",
      "\n",
      "    包含一审程序违法  适当性义务_频次  法律适用规则_频次  责任主体_频次  举证责任分配_频次  告知说明义务_频次  损失赔偿数额_频次  \\\n",
      "0          0       2.0        0.0      0.0        1.0        1.0        0.0   \n",
      "1          0       0.0        0.0      0.0        0.0        0.0        0.0   \n",
      "2          0       3.0        0.0      0.0        0.0        0.0        0.0   \n",
      "3          0       0.0        0.0      0.0        0.0        0.0        0.0   \n",
      "4          0       8.0        0.0      0.0        0.0        1.0        0.0   \n",
      "..       ...       ...        ...      ...        ...        ...        ...   \n",
      "65         0       0.0        0.0      0.0        0.0        0.0        0.0   \n",
      "66         0      10.0        0.0      0.0        0.0        1.0        2.0   \n",
      "67         0       0.0        0.0      0.0        0.0        0.0        0.0   \n",
      "68         0       0.0        0.0      0.0        0.0        0.0        0.0   \n",
      "69         0       0.0        0.0      0.0        0.0        0.0        0.0   \n",
      "\n",
      "    免责事由_频次  九民纪要_频次  \n",
      "0       0.0      0.0  \n",
      "1       0.0      0.0  \n",
      "2       0.0      0.0  \n",
      "3       0.0      0.0  \n",
      "4       0.0      0.0  \n",
      "..      ...      ...  \n",
      "65      0.0      0.0  \n",
      "66      0.0      0.0  \n",
      "67      0.0      0.0  \n",
      "68      0.0      0.0  \n",
      "69      0.0      0.0  \n",
      "\n",
      "[70 rows x 16 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "custom_wordss = [\"适当性义务\", \"法律适用规则\", \"责任主体\", \"举证责任分配\", \"告知说明义务\", \"损失赔偿数额\", \"免责事由\", \"九民纪要\"]\n",
    "\n",
    "# 创建一个包含custom_words的字典，初始频率都为0\n",
    "custom_word_counts = {word: 0 for word in custom_wordss}\n",
    "\n",
    "# 遍历DataFrame的每一行，统计每个词汇的频次并添加到新列\n",
    "for index, row in df.iterrows():\n",
    "    text = row['原文']\n",
    "    for word in custom_wordss:\n",
    "        count = text.count(word)\n",
    "        df.at[index, f'{word}_频次'] = count\n",
    "\n",
    "# 如果词汇在文本中不存在，将缺失值填充为0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# 打印包含custom_words词汇频次的DataFrame\n",
    "print(df)\n",
    "df.to_csv('原文.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(text, topic_dict):\n",
    "    topic_count = {key: text.count(key) for key in topic_dict.keys()}\n",
    "    most_common_topic = max(topic_count, key=topic_count.get, default=None)\n",
    "    return topic_dict.get(most_common_topic, None)\n",
    "\n",
    "# 定义主题字典\n",
    "di = {\"损失赔偿\": 7, \"免责事由\": 6, \"告知说明\": 5, \"举证责任\": 4, \"法律适用\": 3, \"适当性\": 2, \"责任主体\": 1}\n",
    "\n",
    "# 应用函数到DataFrame的列\n",
    "df[\"相关主题\"] = df[\"二审法院_processed\"].apply(lambda x: assign_topic(x, di))\n",
    "df['相关主题'].head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def tongji(text, stopwords):\n",
    "    words_dic = {}\n",
    "    words = jieba.cut(text)\n",
    "    words = [str(w) for w in words if str(w) not in stopwords]\n",
    "    for word in words:\n",
    "        if word not in words_dic:\n",
    "            words_dic[word] = 0\n",
    "        words_dic[word] += 1\n",
    "    return words_dic\n",
    "\n",
    "# 应用tongji函数，统计词频\n",
    "df['word_counts'] = df['二审法院_processed'].apply(lambda x: tongji(x, stopwords))\n",
    "\n",
    "# 添加标签列\n",
    "#di = {\"适当性\": 7, \"法律适用\": 6, \"责任主体\": 5, \"举证责任\": 4, \"告知说明\": 3, \"免责事由\": 2}\n",
    "#df[\"相关主题\"] = df[\"二审法院_processed\"].map(di)\n",
    "\n",
    "# 删除缺失值\n",
    "#df.dropna(subset=[\"二审法院_processed\"], inplace=True)\n",
    "\n",
    "# 打印处理后的数据框信息\n",
    "#df['相关主题'].head\n",
    "\n",
    "#word_counts\n",
    "df.to_csv('原文1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高频词： [('公司', 505), ('合同', 488), ('支行', 375), ('风险', 354), ('基金', 340), ('证据', 332), ('证明', 311), ('产品', 311), ('证券', 307), ('法院', 286), ('投资', 275), ('事实', 261), ('本案', 261), ('借款', 255), ('提供', 247), ('承担', 244), ('损失', 226), ('一审', 222), ('银行', 221), ('约定', 214), ('未', 211), ('原告', 205), ('本院', 201), ('起诉', 191), ('认定', 191), ('购买', 189), ('被告', 184), ('利息', 182), ('协议', 169), ('赔偿', 169), ('账户', 165), ('予以', 161), ('保证', 160), ('应', 157), ('金融', 152), ('义务', 147), ('交易', 140), ('消费者', 139), ('提交', 137), ('情况', 135), ('销售', 133), ('虚假', 132), ('理财产品', 132), ('本金', 132), ('法律', 127), ('请求', 126), ('理财', 126), ('贷款', 124), ('不予', 124), ('客户', 124)]\n",
      "低频词： [('有理', 1), ('处理不当', 1), ('阻止', 1), ('货物', 1), ('自治', 1), ('15696684', 1), ('冰箱', 1), ('电视', 1), ('洗衣机', 1), ('八步区', 1), ('西路', 1), ('案赔', 1), ('元至桂', 1), ('186616878', 1), ('万至', 1), ('36267392', 1), ('向荣盛', 1), ('元之日', 1), ('日即', 1), ('余款', 1), ('桂司', 1), ('产险', 1), ('日才', 1), ('因桂', 1), ('511898', 1), ('理赔金', 1), ('桂向', 1), ('不清', 1), ('三百二十三', 1), ('令', 1), ('一致意见', 1), ('新卡故', 1), ('新卡', 1), ('卡片', 1), ('届满', 1), ('主诉', 1), ('下降', 1), ('由此证明', 1), ('大额', 1), ('几家', 1), ('多家', 1), ('极小', 1), ('续卡', 1), ('基本一致', 1), ('281556', 1), ('失', 1), ('限', 1), ('40222295', 1), ('上然', 1), ('从德胜', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 合并所有文本的词频统计结果，同时排除空字符\n",
    "total_word_counts = Counter()\n",
    "for word_count in df['word_counts']:\n",
    "    total_word_counts.update({word: count for word, count in word_count.items() if word.strip()})\n",
    "\n",
    "# 提取高频词\n",
    "num_high_freq_words = 50  # 您可以根据需要调整这个数字\n",
    "high_freq_words = total_word_counts.most_common(num_high_freq_words)\n",
    "\n",
    "# 提取低频词\n",
    "num_low_freq_words = 50  # 您可以根据需要调整这个数字\n",
    "low_freq_words = total_word_counts.most_common()[:-num_low_freq_words-1:-1]\n",
    "\n",
    "print(\"高频词：\", high_freq_words)\n",
    "print(\"低频词：\", low_freq_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    涉诉 信息 表及 相关 裁判 文书 证明 20182019 年间 建设 集团 四笔 贷款 提...\n",
      "1    材料 证明 本案 诉请 服务 单据 之夫 刑事案件 被告人 未能 法院 刑事案件 档案资料 ...\n",
      "2    材料 相关 证据 真实性 合法性 关联性 予以 确认 案涉 合同 犹豫 期 约定 收悉 合同...\n",
      "3    一国元 证券公司 投资 顾问 公示 微信 截图 来往 港澳 通行证 截图 证据 内容 证券 ...\n",
      "4    理财产品 客户 协议书 身份证 复印件 客户 风险 评估 问卷 认定 错误 导致 事实 认定...\n",
      "Name: 二审法院_processed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 自定义词汇列表\n",
    "custom_words = [\"损失赔偿\", \"免责事由\", \"告知说明\", \"举证责任\", \"法律适用\", \"适当性\", \"责任主体\", \"九民纪要\", \"证券\", \"虚假陈述\", \"侵权\", \"交易\", \"代理\", \"财产\", \"损害\", \"赔偿\", \"金融\", \"委托\", \"理财\", \"融资\", \"融券\"]\n",
    "\n",
    "# 将自定义词汇添加到结巴分词的词典中\n",
    "for word in custom_words:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "# 加载自定义词典\n",
    "#jieba.load_userdict(\"custom.txt\")\n",
    "# 加载停用词\n",
    "def get_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f])\n",
    "    return stopwords\n",
    "\n",
    "# 去除特定的中文姓名\n",
    "def remove_specific_chinese_names(text, common_surnames):\n",
    "    pattern = '|'.join([f'{surname}[\\u4e00-\\u9fa5]{{1,2}}' for surname in common_surnames])\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# 去除特定的地点名称\n",
    "def remove_specific_places(text):\n",
    "    pattern = r'\\w*(市|州|门)'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# 去除标点符号\n",
    "def remove_punctuation(text):\n",
    "    punctuation = r\"\"\"！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\"\"\n",
    "    return re.sub(r\"[%s]+\" % punctuation, \"\", text)\n",
    "\n",
    "# 结巴分词并去除停用词\n",
    "def jieba_cut_remove_stopwords(text, stopwords):\n",
    "    words = jieba.lcut(text)\n",
    "    return ' '.join([word for word in words if word not in stopwords and word.strip()])\n",
    "\n",
    "# 加载停用词\n",
    "stopwords = get_stop_words(\"停用词表.txt\")\n",
    "\n",
    "# 定义常见的姓氏\n",
    "common_surnames = ['王','阚','汪', '李','任', '张', '刘', '陈', '杨', '赵', '黄', '周', '吴', '徐', '孙', '胡', '朱', '高', '林', '何', '郭', '马', '罗',\"康\"]\n",
    "\n",
    "# 应用所有文本处理步骤\n",
    "def process_text(text, stopwords, common_surnames):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_specific_places(text)\n",
    "    text = remove_specific_chinese_names(text, common_surnames)\n",
    "    text = jieba_cut_remove_stopwords(text, stopwords)\n",
    "    return text\n",
    "\n",
    "# 应用函数到DataFrame的列\n",
    "df['二审法院_processed'] = df['二审法院'].apply(lambda x: process_text(x, stopwords, common_surnames))\n",
    "\n",
    "# 输出处理后的结果\n",
    "print(df['二审法院_processed'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    涉诉 信息 相关 裁判 文书 建设 集团 贷款 担保 生效 判决 确认 案件 涉诉 金额 自...\n",
      "1    材料 诉请 服务 刑事案件 未能 刑事案件 服务 案外人 此种 方式 消费者 权益保护法 消...\n",
      "2    材料 相关 真实性 合法性 关联性 确认 案涉 期 发现 提出 解除合同 退还 保费 并未 ...\n",
      "3    证券公司 顾问 微信 截图 截图 内容 重庆 营业部 顾问 目的 工作人员 聊天记录 章 聊...\n",
      "4    理财产品 客户 协议书 身份证 复印件 客户 评估 问卷 错误 导致 错误 新城 蔡甸 履行...\n",
      "Name: 二审法院_processed2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "\n",
    "# 定义函数统计词频\n",
    "def count_words(text_series):\n",
    "    word_counts = Counter()\n",
    "    for text in text_series:\n",
    "        word_counts.update(jieba.lcut(text))\n",
    "    return word_counts\n",
    "\n",
    "# 统计词频\n",
    "word_counts = count_words(df['二审法院_processed'])\n",
    "\n",
    "# 定义过滤函数\n",
    "def filter_words(text, word_counts, limit_min, limit_max):\n",
    "    words = jieba.lcut(text)\n",
    "    return ' '.join([word for word in words if limit_min <= word_counts[word] <= limit_max])\n",
    "\n",
    "# 应用过滤函数\n",
    "limit_min =10\n",
    "limit_max =150\n",
    "df['二审法院_processed2'] = df['二审法院_processed'].apply(lambda x: filter_words(x, word_counts, limit_min, limit_max))\n",
    "\n",
    "# 查看结果\n",
    "print(df['二审法院_processed2'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.009*\"融资\" + 0.008*\"交易\"')\n",
      "(1, '0.013*\"支付\" + 0.009*\"兴业银行\"')\n",
      "(2, '0.015*\"利率\" + 0.014*\"贷款\"')\n",
      "(3, '0.008*\"存款\" + 0.007*\"资金\"')\n",
      "(4, '0.010*\"本金\" + 0.009*\"贷款\"')\n",
      "(5, '0.008*\"义务\" + 0.008*\"消费者\"')\n",
      "(6, '0.010*\"本金\" + 0.008*\"利率\"')\n",
      "(7, '0.012*\"存款\" + 0.009*\"请求\"')\n",
      "(8, '0.013*\"车辆\" + 0.012*\"合作社\"')\n",
      "(9, '0.010*\"担保\" + 0.008*\"信托\"')\n",
      "(10, '0.021*\"虚假\" + 0.015*\"行政处罚\"')\n",
      "(11, '0.013*\"销售\" + 0.011*\"客户\"')\n",
      "(12, '0.011*\"佣金\" + 0.010*\"收取\"')\n",
      "(13, '0.009*\"理财产品\" + 0.008*\"管理\"')\n",
      "(14, '0.019*\"融资\" + 0.016*\"交易\"')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# 计算主题\n",
    "documents = df['二审法院_processed2'].tolist()\n",
    "documents = [str(i).split(\" \") for i in documents]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "num_topics =15# 假设最佳主题个数\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "# lda模型，num_topics设置主题的个数\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "# 打印所有主题，每个主题显示5个词\n",
    "for topic in lda.print_topics(num_words=2):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题 0: 4.20%\n",
      "主题 1: 7.23%\n",
      "主题 2: 5.89%\n",
      "主题 3: 6.09%\n",
      "主题 4: 4.94%\n",
      "主题 5: 14.41%\n",
      "主题 6: 2.53%\n",
      "主题 7: 6.73%\n",
      "主题 8: 3.39%\n",
      "主题 9: 5.10%\n",
      "主题 10: 15.14%\n",
      "主题 11: 3.59%\n",
      "主题 12: 9.72%\n",
      "主题 13: 2.98%\n",
      "主题 14: 8.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 假设documents, dictionary, corpus, 和 lda已经定义好了\n",
    "# documents = df['二审法院_processed2'].tolist()\n",
    "# documents = [str(i).split(\" \") for i in documents]\n",
    "# dictionary = corpora.Dictionary(documents)\n",
    "# corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "# lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "# 获取每个文档的主题分布\n",
    "doc_topics = lda.get_document_topics(corpus, minimum_probability=0)\n",
    "\n",
    "# 累加每个主题的概率\n",
    "topic_totals = [0] * num_topics  # 假设您有num_topics个主题\n",
    "for doc in doc_topics:\n",
    "    for topic, prob in doc:\n",
    "        topic_totals[topic] += prob\n",
    "\n",
    "# 计算总概率以便转换为百分比\n",
    "total_probability = sum(topic_totals)\n",
    "topic_percentages = [prob / total_probability * 100 for prob in topic_totals]\n",
    "\n",
    "# 打印主题的百分比\n",
    "for i, percentage in enumerate(topic_percentages):\n",
    "    print(f\"主题 {i}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题 0 的累加概率: 2.9859778713762353\n",
      "主题 1 的累加概率: 5.064116954628844\n",
      "主题 2 的累加概率: 4.109571607907128\n",
      "主题 3 的累加概率: 4.263321213202289\n",
      "主题 4 的累加概率: 3.395369908776047\n",
      "主题 5 的累加概率: 10.112266252453992\n",
      "主题 6 的累加概率: 1.8225906629850215\n",
      "主题 7 的累加概率: 4.6975700428229175\n",
      "主题 8 的累加概率: 2.3793930665124208\n",
      "主题 9 的累加概率: 3.605410030548228\n",
      "主题 10 的累加概率: 10.605507950716856\n",
      "主题 11 的累加概率: 2.5105676578896237\n",
      "主题 12 的累加概率: 6.780816793376289\n",
      "主题 13 的累加概率: 2.0496791182922607\n",
      "主题 14 的累加概率: 5.617840770621115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 假设documents, dictionary, corpus, 和 lda已经定义好了\n",
    "\n",
    "# 获取每个文档的主题分布\n",
    "doc_topics = lda.get_document_topics(corpus, minimum_probability=0)\n",
    "\n",
    "# 累加每个主题的概率\n",
    "topic_totals = [0] * num_topics  # 假设您有num_topics个主题\n",
    "for doc in doc_topics:\n",
    "    for topic, prob in doc:\n",
    "        topic_totals[topic] += prob\n",
    "\n",
    "# 打印每个主题的累加概率\n",
    "for i, total in enumerate(topic_totals):\n",
    "    print(f\"主题 {i} 的累加概率: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# 保存到本地\n",
    "df_temp2 = df.copy()\n",
    "# # 为每个文档提取主题、词汇和概率，并添加到 DataFrame 中\n",
    "\n",
    "df_temp2['lda_topic'] = df_temp2['二审法院_processed'].apply(lambda x: lda[dictionary.doc2bow(x.split())])\n",
    "df_temp2[['topic', 'probability']] = pd.DataFrame(df_temp2['lda_topic'].apply(lambda x: max(x, key=lambda item: item[1])).tolist(), index=df.index)\n",
    "df_temp2['topic_words'] = df_temp2['lda_topic'].apply(lambda x: [word for word, prob in lda.show_topic(max(x, key=lambda item: item[1])[0])])\n",
    "\n",
    "# 将修改后的 DataFrame 保存到 Excel 文件\n",
    "\n",
    "# 创建一个函数，用于获取文档的主题得分\n",
    "def get_topic_scores(text):\n",
    "    bow = dictionary.doc2bow(text.split())\n",
    "    topic_scores = {topic_id: score for topic_id, score in lda.get_document_topics(bow)}\n",
    "    return topic_scores\n",
    "\n",
    "# 应用函数到DataFrame的列，获取各个文档的主题得分\n",
    "df_temp2['topic_scores'] = df_temp2['二审法院_processed2'].apply(get_topic_scores)\n",
    "\n",
    "# 将主题得分添加到DataFrame中\n",
    "for topic_id in range(num_topics):\n",
    "    df_temp2[f'topic_{topic_id}_score'] = df_temp2['topic_scores'].apply(lambda x: x.get(topic_id, 0.0))\n",
    "\n",
    "# 保存修改后的DataFrame到Excel文件\n",
    "df_temp2.to_csv(\"原文.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.063*\"账户\" + 0.052*\"虚假\" + 0.050*\"融资\"')\n",
      "(1, '0.061*\"利息\" + 0.049*\"认定\" + 0.049*\"存款\"')\n",
      "(2, '0.065*\"保证\" + 0.059*\"协议\" + 0.054*\"担保\"')\n",
      "(3, '0.086*\"被告\" + 0.076*\"起诉\" + 0.047*\"赔偿\"')\n",
      "(4, '0.065*\"交易\" + 0.057*\"协议\" + 0.054*\"销售\"')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# 计算主题\n",
    "documents = df['二审法院_processed2'].tolist()\n",
    "documents = [str(i).split(\" \") for i in documents]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "num_topics =5# 假设最佳主题个数\n",
    "dictionary2 = corpora.Dictionary(documents)\n",
    "corpus2 = [dictionary.doc2bow(words) for words in documents]\n",
    "# lda模型，num_topics设置主题的个数\n",
    "lda2 = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "# 打印所有主题，每个主题显示5个词\n",
    "for topic in lda2.print_topics(num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.056*\"利息\" + 0.042*\"贷款\" + 0.040*\"账户\" + 0.040*\"支付\" + 0.040*\"本金\" + 0.038*\"金融\" + 0.035*\"利率\"')\n",
      "(1, '0.072*\"融资\" + 0.041*\"购买\" + 0.039*\"交易\" + 0.038*\"账户\" + 0.038*\"金融\" + 0.037*\"存款\" + 0.037*\"消费者\"')\n",
      "(2, '0.068*\"协议\" + 0.045*\"保证\" + 0.042*\"认定\" + 0.041*\"担保\" + 0.034*\"购买\" + 0.033*\"理财\" + 0.033*\"法律\"')\n",
      "(3, '0.070*\"被告\" + 0.065*\"购买\" + 0.057*\"起诉\" + 0.047*\"虚假\" + 0.041*\"赔偿\" + 0.040*\"提交\" + 0.038*\"予以\"')\n",
      "(4, '0.111*\"起诉\" + 0.091*\"虚假\" + 0.086*\"比例\" + 0.052*\"予以\" + 0.050*\"认定\" + 0.048*\"赔偿\" + 0.037*\"情况\"')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# 计算主题\n",
    "documents = df['二审法院_processed2'].tolist()\n",
    "documents = [str(i).split(\" \") for i in documents]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "num_topics =5# 假设最佳主题个数\n",
    "dictionary3 = corpora.Dictionary(documents)\n",
    "corpus3 = [dictionary.doc2bow(words) for words in documents]\n",
    "# lda模型，num_topics设置主题的个数\n",
    "lda3 = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "# 打印所有主题，每个主题显示5个词\n",
    "for topic in lda3.print_topics(num_words=7):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pandas\n",
    "print(pyLDAvis.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手肘法计算主题个数\n",
    "max_topics = 50\n",
    "documents = df['二审法院_processed2'].tolist()\n",
    "documents = [str(i).split(\" \") for i in documents]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "# 使用手肘法确定最佳主题个数\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(2, max_topics):  # 调整测试的主题个数范围\n",
    "    lda_model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "    model_list.append(lda_model)\n",
    "    coherence_model = models.CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values.append(coherence_model.get_coherence())\n",
    "# 画出手肘法的曲线\n",
    "x = range(2, max_topics)  # 主题个数范围\n",
    "plt.plot(x, coherence_values, marker='o')\n",
    "plt.xlabel(\"主题个数\")\n",
    "plt.ylabel(\"一致性得分\")\n",
    "plt.title(\"手肘法确定最佳主题个数\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算主题\n",
    "documents = df['二审法院_processed'].tolist()\n",
    "documents = [str(i).split(\" \") for i in documents]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "num_topics = 7 # 假设最佳主题个数\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "# lda模型，num_topics设置主题的个数\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "# 打印所有主题，每个主题显示5个词\n",
    "for topic in lda.print_topics(num_words=10):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置图表样式\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 选择一个主题进行可视化\n",
    "topic_num = 0  # 可以更改为您想要可视化的主题编号\n",
    "topic_words = df_temp2[df_temp2['topic'] == topic_num]['topic_words'].iloc[0]\n",
    "probabilities = [prob for _, prob in lda.show_topic(topic_num)]\n",
    "\n",
    "# 创建条形图\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=probabilities, y=topic_words)\n",
    "plt.title(f'Topic {topic_num} Word Distribution')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# 保存到本地\n",
    "df_temp2 = df.copy()\n",
    "# # 为每个文档提取主题、词汇和概率，并添加到 DataFrame 中\n",
    "\n",
    "df_temp2['lda_topic'] = df_temp2['二审法院_processed'].apply(lambda x: lda3[dictionary.doc2bow(x.split())])\n",
    "df_temp2[['topic', 'probability']] = pd.DataFrame(df_temp2['lda_topic'].apply(lambda x: max(x, key=lambda item: item[1])).tolist(), index=df.index)\n",
    "df_temp2['topic_words'] = df_temp2['lda_topic'].apply(lambda x: [word for word, prob in lda3.show_topic(max(x, key=lambda item: item[1])[0])])\n",
    "\n",
    "# 将修改后的 DataFrame 保存到 Excel 文件\n",
    "\n",
    "# 创建一个函数，用于获取文档的主题得分\n",
    "def get_topic_scores(text):\n",
    "    bow = dictionary.doc2bow(text.split())\n",
    "    topic_scores = {topic_id: score for topic_id, score in lda3.get_document_topics(bow)}\n",
    "    return topic_scores\n",
    "\n",
    "# 应用函数到DataFrame的列，获取各个文档的主题得分\n",
    "df_temp2['topic_scores'] = df_temp2['二审法院_processed2'].apply(get_topic_scores)\n",
    "\n",
    "# 将主题得分添加到DataFrame中\n",
    "for topic_id in range(num_topics):\n",
    "    df_temp2[f'topic_{topic_id}_score'] = df_temp2['topic_scores'].apply(lambda x: x.get(topic_id, 0.0))\n",
    "\n",
    "# 保存修改后的DataFrame到Excel文件\n",
    "df_temp2.to_excel(\"output_lda.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# 确保您有LDA模型（lda）、语料库（corpus）和词典（dictionary）\n",
    "# lda = ...  # 您的LDA模型\n",
    "# corpus = ...  # 您的语料库\n",
    "# dictionary = ...  # 您的词典\n",
    "\n",
    "# 准备pyLDAvis数据\n",
    "vis_data = gensimvis.prepare(df_temp2['lda_topic'], corpus, dictionary)\n",
    "\n",
    "# 显示可视化结果（在Jupyter Notebook中）\n",
    "pyLDAvis.display(vis_data)\n",
    "\n",
    "# 或者保存为HTML文件\n",
    "pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_temp2['topic'] = [topic[0] for topic in topics_info]\n",
    "df_temp2['topic_words'] =  [lda.show_topic(topic[0][0]) for topic in top_topics]\n",
    "df_temp2['topic_probability'] = [topic for topic in topics_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 'SimHei' 是黑体的字体名\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "# 或者使用 FontProperties 设置特定的字体\n",
    "# font = FontProperties(fname='/path/to/your/font.ttf', size=14)\n",
    "\n",
    "# 创建条形图\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=probabilities, y=topic_words)\n",
    "plt.title(f'Topic {topic_num} Word Distribution', fontsize=14)\n",
    "plt.xlabel('Probability', fontsize=12)\n",
    "plt.ylabel('Words', fontsize=12)\n",
    "# 如果使用 FontProperties，需要在每个涉及文本的地方设置字体\n",
    "# plt.title(f'Topic {topic_num} Word Distribution', fontproperties=font)\n",
    "# plt.xlabel('Probability', fontproperties=font)\n",
    "# plt.ylabel('Words', fontproperties=font)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "#在notebook显示\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#获取pyldavis需要的参数\n",
    "topic_term_dists = np.stack([slda.get_topic_word_dist(k) for k in range(slda.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in slda.docs])\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "doc_lengths = np.array([len(doc.words) for doc in slda.docs])\n",
    "vocab = list(slda.used_vocabs)\n",
    "term_frequency = slda.used_vocab_freq\n",
    "\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency,\n",
    "    start_index=0, # tomotopy话题id从0开始，pyLDAvis话题id从1开始\n",
    "    sort_topics=False #注意：否则pyLDAvis与tomotopy内的话题无法一一对应。 \n",
    ")\n",
    "#可视化结果存到html文件中\n",
    "pyLDAvis.save_html(prepared_data, './res/sldavis.html')\n",
    "\n",
    "#notebook中显示\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pyLDAvis pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "# 加载停用词\n",
    "stop_list = []\n",
    "with open('D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_list.append(line.strip())  # 使用strip()去除换行符\n",
    "\n",
    "# 假设您的DataFrame名为df，且包含一个名为'原文'的列\n",
    "for idx, row in df.iterrows():\n",
    "    words = jieba.cut(str(row['原文']))  # 使用jieba.cut进行分词\n",
    "    filtered_words = [word for word in words if word not in stop_list and not word.isdigit()] \n",
    "     # 过滤停用词\n",
    "    df.at[idx, '分词结果'] = ' '.join(filtered_words)  # 将分词结果保存回DataFrame\n",
    "\n",
    "# 如果需要查看分词结果，可以直接打印df或者保存到文件\n",
    "print(df.分词结果)\n",
    "\n",
    "\n",
    "# 去掉空字符\n",
    "def delete_r_n(line):\n",
    "    return line.replace('\\r', '').replace('\\n', '').strip()\n",
    "\n",
    "# 加载停用词\n",
    "def get_stop_words(stop_words_dir):\n",
    "    stop_words = []\n",
    "\n",
    "    with open(stop_words_dir, 'r', encoding='utf-8') as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = delete_r_n(line)\n",
    "            stop_words.append(line)\n",
    "\n",
    "    stop_words = set(stop_words)\n",
    "    return stop_words\n",
    "\n",
    "# 加载停用词,stopwords为停用词列表\n",
    "stopwords = get_stop_words(r'停用词表.txt')\n",
    "\n",
    "#stopwords = get_stop_words(r'D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt')\n",
    "\n",
    "def jieba_cut_remove_stopwords(text):\n",
    "    \"\"\"结巴分词，并去除停用词\n",
    "    \"\"\"\n",
    "    words = jieba.lcut(text)\n",
    "    results = []\n",
    "    for word in words:\n",
    "        # 去除停用词和空元素\n",
    "        if word not in stopwords and word.strip():\n",
    "            results.append(word)\n",
    "    return ' '.join(results)\n",
    "\n",
    "df['分析结果'] = df[\"分词结果\"].apply(lambda x:jieba_cut_remove_stopwords(str(x)))\n",
    "\n",
    "df['分析结果'].head\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "\n",
    "# 假设df是您的DataFrame，'上诉人理由'是包含文本的列名\n",
    "# 这里需要先对文本进行分词处理\n",
    "sentences = [list(jieba.cut(text)) for text in df['上诉人理由']]\n",
    "\n",
    "# 创建Word2Vec模型\n",
    "model = Word2Vec(sentences=sentences, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 寻找与“合同”最相似的词汇\n",
    "similar_words = model.wv.most_similar('合同', topn=20)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 去掉空字符\n",
    "def delete_r_n(line):\n",
    "    return line.replace('\\r', '').replace('\\n', '').strip()\n",
    "\n",
    "# 加载停用词\n",
    "def get_stop_words(stop_words_dir):\n",
    "    stop_words = []\n",
    "\n",
    "    with open(stop_words_dir, 'r', encoding='utf-8') as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = delete_r_n(line)\n",
    "            stop_words.append(line)\n",
    "\n",
    "    stop_words = set(stop_words)\n",
    "    return stop_words\n",
    "\n",
    "# 加载停用词,stopwords为停用词列表\n",
    "stopwords = get_stop_words(r'停用词表.txt')\n",
    "\n",
    "#stopwords = get_stop_words(r'D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt')\n",
    "\n",
    "def jieba_cut_remove_stopwords(text):\n",
    "    \"\"\"结巴分词，并去除停用词\n",
    "    \"\"\"\n",
    "    words = jieba.lcut(text)\n",
    "    results = []\n",
    "    for word in words:\n",
    "        # 去除停用词和空元素\n",
    "        if word not in stopwords and word.strip():\n",
    "            results.append(word)\n",
    "    return ' '.join(results)\n",
    "\n",
    "df['分析结果'] = df[\"分词结果\"].apply(lambda x:jieba_cut_remove_stopwords(str(x)))\n",
    "\n",
    "df['分析结果'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 定义正则表达式\n",
    "pattern = {\n",
    "    '上诉人':  r'上诉人.*?：(.*?)。',\n",
    "    '被上诉人': r'被上诉人.*?：(.*?)。',\n",
    "    '原审被告': r'原审被告.*?：(.*?)。',\n",
    "    '原审原告': r'原审原告.*?：(.*?)。'\n",
    "}\n",
    "\n",
    "# 提取信息\n",
    "def extract_info(text):\n",
    "    info = {key: re.search(pattern[key], text).group(1) if re.search(pattern[key], text) else '' for key in pattern}\n",
    "    return info\n",
    "\n",
    "# 判断上诉人是否为个体\n",
    "def is_individual(appellant):\n",
    "    return 1 if any(gender in appellant for gender in ['男', '女']) else 0\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['提取信息'] = df['原文'].apply(extract_info)\n",
    "df['上诉人是否个体'] = df['提取信息'].apply(lambda x: is_individual(x['上诉人']))\n",
    "\n",
    "print(df[['提取信息', '上诉人是否个体']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['原文']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_result(text):\n",
    "    return 1 if \"维持原判\" in text else 0\n",
    "\n",
    "df['审判结果'] = df['原文'].apply(judge_result)\n",
    "print(df[['审判结果', '上诉人是否个体']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 定义正则表达式\n",
    "pattern = r'本案(?!.*不).*?为(.*?)纠纷'\n",
    "\n",
    "# 提取纠纷类型\n",
    "def extract_type(text):\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['纠纷类型'] = df['原文'].apply(extract_type)\n",
    "\n",
    "print(df[['纠纷类型', '上诉人是否个体']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_facts_and_reasons(text):\n",
    "    # 正则表达式匹配不同表述的“事实和理由”部分\n",
    "    pattern = r'(事实和理由：|事实与理由：|上诉理由是：|事实与理由:|上诉人于某上诉称，|事实及理由：|事实及理由：|事实和理由:|。请求：|宣判后，张小喜不服，向本院提起上诉称：|上诉人农行登峰支行不服原审判决，向本院提起上诉称,|事实和理由:|上诉人林娟的上诉及答辩理由为：)(.*?)(?:\\n\\n|\\Z)'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(2).strip()  # 返回第二个捕获组的内容\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['事实和理由2'] = df['原文'].apply(extract_facts_and_reasons)\n",
    "\n",
    "print(df['事实和理由2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'(二审.*?证据|本院二审.*?提供|本院认为：《国务院关于清理整顿各类交易场所切实防范金融风险的决定》|本院认为：郑桁在一审时|二审经审理查明，|除原审查明事实部分的内容，本院二审说明如下事实：|本案二审.*?提交.*?证据)(.*?)依照《中华人民共和国民法.*?，判决如下'\n",
    "\n",
    "# 提取纠纷类型\n",
    "def extract_facts(text):\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['二审法院'] = df['原文'].apply(extract_facts)\n",
    "\n",
    "print(df['二审法院'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "# 加载停用词\n",
    "stop_list = []\n",
    "with open('D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_list.append(line.strip())  # 使用strip()去除换行符\n",
    "\n",
    "# 假设您的DataFrame名为df，且包含一个名为'原文'的列\n",
    "for idx, row in df.iterrows():\n",
    "    words = jieba.cut(str(row['事实和理由2']))  # 使用jieba.cut进行分词\n",
    "    filtered_words = [word for word in words if word not in stop_list and not word.isdigit()] \n",
    "     # 过滤停用词\n",
    "    df.at[idx, '事实和理由2'] = ' '.join(filtered_words)  # 将分词结果保存回DataFrame\n",
    "\n",
    "# 如果需要查看分词结果，可以直接打印df或者保存到文件\n",
    "print(df.事实和理由2)\n",
    "\n",
    "\n",
    "# 去掉空字符\n",
    "def delete_r_n(line):\n",
    "    return line.replace('\\r', '').replace('\\n', '').strip()\n",
    "\n",
    "# 加载停用词\n",
    "def get_stop_words(stop_words_dir):\n",
    "    stop_words = []\n",
    "\n",
    "    with open(stop_words_dir, 'r', encoding='utf-8') as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = delete_r_n(line)\n",
    "            stop_words.append(line)\n",
    "\n",
    "    stop_words = set(stop_words)\n",
    "    return stop_words\n",
    "\n",
    "# 加载停用词,stopwords为停用词列表\n",
    "stopwords = get_stop_words(r'停用词表.txt')\n",
    "\n",
    "#stopwords = get_stop_words(r'D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt')\n",
    "\n",
    "def jieba_cut_remove_stopwords(text):\n",
    "    \"\"\"结巴分词，并去除停用词\n",
    "    \"\"\"\n",
    "    words = jieba.lcut(text)\n",
    "    results = []\n",
    "    for word in words:\n",
    "        # 去除停用词和空元素\n",
    "        if word not in stopwords and word.strip():\n",
    "            results.append(word)\n",
    "    return ' '.join(results)\n",
    "\n",
    "df['事实与理由分析结果'] = df[\"事实和理由2\"].apply(lambda x:jieba_cut_remove_stopwords(str(x)))\n",
    "\n",
    "df['事实与理由分析结果'].head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "# 加载停用词\n",
    "stop_list = []\n",
    "with open('D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_list.append(line.strip())  # 使用strip()去除换行符\n",
    "\n",
    "# 假设您的DataFrame名为df，且包含一个名为'原文'的列\n",
    "for idx, row in df.iterrows():\n",
    "    words = jieba.cut(str(row['事实和理由2']))  # 使用jieba.cut进行分词\n",
    "    filtered_words = [word for word in words if word not in stop_list and not word.isdigit()] \n",
    "     # 过滤停用词\n",
    "    df.at[idx, '事实和理由2'] = ' '.join(filtered_words)  # 将分词结果保存回DataFrame\n",
    "\n",
    "# 如果需要查看分词结果，可以直接打印df或者保存到文件\n",
    "print(df.事实和理由2)\n",
    "\n",
    "\n",
    "# 去掉空字符\n",
    "def delete_r_n(line):\n",
    "    return line.replace('\\r', '').replace('\\n', '').strip()\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_specific_chinese_names(text):\n",
    "    # 定义常见的姓氏\n",
    "    common_surnames = ['王','左','汪','吕','肖','汤','熊', '陶','李', '张', '梁','刘', '阚','康','陈', '杨', '赵', '黄', '周', '吴', '徐', '孙', '胡', '朱', '高', '林', '何', '郭', '马', '罗']\n",
    "    # 构建正则表达式\n",
    "    pattern = '|'.join([f'{surname}[\\u4e00-\\u9fa5]{{1,2}}' for surname in common_surnames])\n",
    "    return re.sub(pattern, '', text)\n",
    "import re\n",
    "\n",
    "# 定义一个函数来去除文本中的特定地点名称\n",
    "def remove_specific_places(text):\n",
    "    # 匹配以“市”、“州”、“门”结尾的词语\n",
    "    pattern = r'\\w*(市|州|门)'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['事实与理由分析结果2'] = df['事实与理由分析结果'].apply(remove_specific_places)\n",
    "\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列\n",
    "df['理由'] = df['事实与理由分析结果2'].apply(remove_specific_chinese_names)\n",
    "\n",
    "stopwords = get_stop_words(r'停用词表.txt')\n",
    "\n",
    "#stopwords = get_stop_words(r'D:\\python_code\\Textanalysis\\文本分析\\stopwords-master\\cn_stopwords.txt')\n",
    "\n",
    "def jieba_cut_remove_stopwords(text):\n",
    "    \"\"\"结巴分词，并去除停用词\n",
    "    \"\"\"\n",
    "    words = jieba.lcut(text)\n",
    "    results = []\n",
    "    for word in words:\n",
    "        # 去除停用词和空元素\n",
    "        if word not in stopwords and word.strip():\n",
    "            results.append(word)\n",
    "    return ' '.join(results)\n",
    "\n",
    "df['上诉人理由'] = df[\"理由\"].apply(lambda x:jieba_cut_remove_stopwords(str(x)))\n",
    "#def remove_empty_strings(words_list):\n",
    "    # 去除空字符串\n",
    "#    return [word for word in words_list if word.strip()]\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'分词结果'的列，该列包含了分词后的词汇列表\n",
    "#df['事实与理由'] = df['事实与理由'].apply(remove_empty_strings)\n",
    "print(df['上诉人理由'])\n",
    "# 现在，您可以继续使用处理过的数据进行LDA模型分析\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import jieba\n",
    "for text in df['上诉人理由']:\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Found non-string value: {text}\")\n",
    "\n",
    "#你可以看到non-string:nan，这个意思是代表着这个不可以用，空白了这一行，去除了停用词之后，所以要去掉，注意回复和诉求都要检查\n",
    "\n",
    "\n",
    "# Remove rows with NaN values in the '诉求_cut' column\n",
    "#df = df.dropna(subset=['事实与理由'])\n",
    "\n",
    "# Now, you can safely check the data type\n",
    "for text in df['上诉人理由']:\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Found non-string value: {text}\")\n",
    "\n",
    "# 分词，这也不是分词，是列矩阵\n",
    "texts2 = [[word for word in text.split(' ')] for text in df['上诉人理由']]\n",
    "# 构建词典\n",
    "dictionary = corpora.Dictionary(texts2)\n",
    "\n",
    "# 使用上面的词典，将转换文档列表（语料）变成 DT 矩阵\n",
    "corpus = [dictionary.doc2bow(text) for text in texts2]\n",
    "\n",
    "texts2, dictionary, corpus\n",
    "\n",
    "\n",
    "# 3. 训练模型\n",
    "lda_model = LdaModel(corpus, num_topics=8, id2word=dictionary, passes=15)\n",
    "\n",
    "# 4. 展示结果\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "\n",
    "# 假设df是您的DataFrame，'上诉人理由'是包含文本的列名\n",
    "# 这里需要先对文本进行分词处理\n",
    "sentences = [list(jieba.cut(text)) for text in df['上诉人理由']]\n",
    "\n",
    "# 创建Word2Vec模型\n",
    "model = Word2Vec(sentences=sentences, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 寻找与“合同”最相似的词汇\n",
    "similar_words = model.wv.most_similar('合同', topn=20)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设df是您的DataFrame，并且它有一个名为'原文'的列和一个名为'上诉人是否个体'的列\n",
    "\n",
    "# 分别提取两组数据\n",
    "group_1 = df[df['上诉人是否个体'] == 1]['上诉人理由']\n",
    "group_0 = df[df['上诉人是否个体'] == 0]['上诉人理由']\n",
    "\n",
    "# 对文本进行预处理（例如分词等）\n",
    "# 这里需要根据您的具体情况来编写预处理代码\n",
    "\n",
    "# 训练词向量模型\n",
    "model_1 = Word2Vec(group_1, size=100, window=5, min_count=1, workers=4)\n",
    "model_0 = Word2Vec(group_0, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 寻找与“合同”最相近的词语\n",
    "similar_words_1 = model_1.wv.most_similar('合同', topn=10)\n",
    "similar_words_0 = model_0.wv.most_similar('合同', topn=10)\n",
    "\n",
    "# 可视化\n",
    "# 这里需要根据您想要的可视化效果来编写代码\n",
    "# 例如，您可以使用matplotlib来绘制条形图或散点图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import jieba\n",
    "for text in df['事实与理由']:\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Found non-string value: {text}\")\n",
    "\n",
    "#你可以看到non-string:nan，这个意思是代表着这个不可以用，空白了这一行，去除了停用词之后，所以要去掉，注意回复和诉求都要检查\n",
    "\n",
    "\n",
    "# Remove rows with NaN values in the '诉求_cut' column\n",
    "#dfs = df.dropna(subset=['事实与理由'])\n",
    "\n",
    "# Now, you can safely check the data type\n",
    "for text in df['事实与理由']:\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Found non-string value: {text}\")\n",
    "\n",
    "# 分词，这也不是分词，是列矩阵\n",
    "texts2 = [[word for word in text.split(' ')] for text in df['事实与理由']]\n",
    "# 构建词典\n",
    "dictionary = corpora.Dictionary(texts2)\n",
    "\n",
    "# 使用上面的词典，将转换文档列表（语料）变成 DT 矩阵\n",
    "corpus = [dictionary.doc2bow(text) for text in texts2]\n",
    "\n",
    "texts2, dictionary, corpus\n",
    "\n",
    "\n",
    "# 3. 训练模型\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# 4. 展示结果\n",
    "topics = lda_model.print_topics(num_words=8)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###查看每个文档主题分布\n",
    "import pandas as pd\n",
    "\n",
    "doc_topic_distributions = [lda_model[bow] for bow in corpus]\n",
    "fixed_distributions = []\n",
    "\n",
    "for distribution in doc_topic_distributions:\n",
    "    fixed_distribution = [0] * lda_model.num_topics\n",
    "    for topic_id, prob in distribution:\n",
    "        fixed_distribution[topic_id] = prob\n",
    "    fixed_distributions.append(fixed_distribution)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the fixed_distributions list into a DataFrame\n",
    "df = pd.DataFrame(fixed_distributions, columns=[f\"Topic_{i}\" for i in range(lda_model.num_topics)])\n",
    "df.to_csv('topic_distributions.csv', index=False)\n",
    "\n",
    "\n",
    "df.head()  # Display the first few rows of the dataframe for review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from collections import defaultdict\n",
    "#it will be slow and slow\n",
    "\n",
    "# 3. 训练LDA模型\n",
    "topic_range = list(range(5,11))\n",
    "ldamodels = {k: LdaModel(corpus, num_topics=k, id2word=dictionary, passes=15) for k in topic_range}\n",
    "\n",
    "# 4. 计算Coherence\n",
    "coherence_values = {}\n",
    "for k, model in ldamodels.items():\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts2, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[k] = coherence_model.get_coherence()\n",
    "\n",
    "# 5. 计算Diversity\n",
    "def compute_diversity(model):\n",
    "    num_topics = model.num_topics\n",
    "    topics = [model.show_topic(i) for i in range(num_topics)]\n",
    "    word_sets = [set([word for word, _ in topic]) for topic in topics]\n",
    "    \n",
    "    total_pairs = num_topics * (num_topics - 1) / 2\n",
    "    overlapping_word_count = sum(len(word_sets[i].intersection(word_sets[j])) for i in range(num_topics) for j in range(i+1, num_topics))\n",
    "    \n",
    "    # diversity is defined as 1 minus average overlap\n",
    "    diversity = 1 - overlapping_word_count / (5 * total_pairs)  # each topic has 10 words by default you should write that 算了我英文不好，就是说你有多少个词做实验就要用多少个\n",
    "    return diversity\n",
    "\n",
    "diversity_values = {k: compute_diversity(model) for k, model in ldamodels.items()}\n",
    "\n",
    "coherence_values, diversity_values\n",
    "#5个词是6比较好，10个呢？10个词语是10个比较好，那我还是选择5个吧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制Coherence和Diversity值的图表\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Coherence折线图，越相近越好，表示主题内部词汇的关系，内部相似度\n",
    "plt.plot(list(coherence_values.keys()), list(coherence_values.values()), '-o', label=\"Coherence\", color='pink')\n",
    "\n",
    "# Diversity折线图，多样性不够多区分度不大\n",
    "plt.plot(list(diversity_values.keys()), list(diversity_values.values()), '-o', label=\"Diversity\", color='green')\n",
    "\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Coherence and Diversity Scores for Different Number of Topics\")\n",
    "plt.xticks(topic_range)\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('原文.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
